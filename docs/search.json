[
  {
    "objectID": "why.html",
    "href": "why.html",
    "title": "Why Tergite?",
    "section": "",
    "text": "Tergite was built to enable multiple users across the world interface with a single computer in one corner of the world. This was because it was not practical for private individuals to purchase and operate their own quantum computers. At least not at the present time.\nIt is impractical for private individuals to own and operate their own quantum computers because of the following reasons:\n\nQuantum Computers are too big to be personal\nThe superconducting quantum computers built under the WACQT program are very big. They typically occupy space of 3 square metres and more.\n\n\n\n\n\n\n\n\n\nInstalling the WACQT quantum computer\n\n\n\n\n\n\n\nQuantum Computers are too complex to be personal\nThe WACQT quantum computer is a complex machine. It requires temperatures of -273oC for superconductivity to kick in. Superconductivity is where electrical conductors offer zero resistance to the flow of electricity. The quantum mechanics of the system are observable and controllable when superconductivity has kicked in.\nSuch temperatures can only be achieved with huge fridges that require complex setups of compressors and other equipment that require professional setup and operation.\nApart from the required temperatures, the quantum computer is controlled by use of radio frequency (RF) pulses generated by specialized RF equipment. Setting these up as well as operating them requires a skillset that is only possessed by a few people, after years of training.\n\n\nQuantum Computers are too expensive to be personal\nSince the quantum computer requires a number of expensive equipment as well as specialist labour, it is quite expensive to purchase and operate. Even small companies cannot afford it.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "tutorials/07_puhuri_tergite_flows.html",
    "href": "tutorials/07_puhuri_tergite_flows.html",
    "title": "Puhuri-Tergite playground",
    "section": "",
    "text": "This is just a playgorund as we try to connect Tergite to Puhuri",
    "crumbs": [
      "Tutorials",
      "Puhuri-Tergite playground"
    ]
  },
  {
    "objectID": "tutorials/07_puhuri_tergite_flows.html#global-variables",
    "href": "tutorials/07_puhuri_tergite_flows.html#global-variables",
    "title": "Puhuri-Tergite playground",
    "section": "GLOBAL VARIABLES",
    "text": "GLOBAL VARIABLES\nThe following gloabl variables should be set to the right values.\nimport asyncio\nimport enum\nimport pprint\n\nimport pydantic\n\nfrom datetime import datetime, timezone\nfrom typing import Any, Dict, Optional, Tuple, List\n\nfrom motor import motor_asyncio\nfrom pymongo import UpdateOne\nfrom waldur_client import WaldurClient, ComponentUsage\n\n# The User-set variables\nWALDUR_URI = \"https://access.nordiquest.net/api/\"\nWALDUR_TOKEN = \"&lt;API key of your user, click on the user in the navbar&gt;\"\nPROVIDER_UUID = \"&lt;Unique ID of the Service provider for QAL900, visit the service provider detail page and get the uuid in the URL box&gt;\"\n# this requires one to install mongodb. Or you can set it to \"\" or None and it will be ignored\nMONGODB_URI = \"mongodb://localhost:27017\"\n\n\n# Puhuri Waldur client\nCLIENT = WaldurClient(WALDUR_URI, WALDUR_TOKEN)\nDB_CLIENT: Optional[motor_asyncio.AsyncIOMotorClient] = None\nif MONGODB_URI:\n    DB_CLIENT = motor_asyncio.AsyncIOMotorClient(MONGODB_URI)\n    DB_NAME = \"your-database\"\n    PROJECTS_COLLECTION = \"projects\"\n\n# Exceptions\n\nclass BaseQal9000Exception(Exception):\n    def __init__(self, message: str = \"\"):\n        self._message = message\n\n    def __repr__(self):\n        return f\"{self.__class__.__name__}: {self._message}\"\n\n    def __str__(self):\n        return self._message if self._message else self.__class__.__name__\n\nclass ResourceNotFoundError(BaseQal9000Exception):\n    \"\"\"Raised when no resources are found\"\"\"\n\nclass ComponentNotFoundError(BaseQal9000Exception):\n    \"\"\"Raised when no component is found\"\"\"\n\nclass PlanPeriodNotFoundError(BaseQal9000Exception):\n    \"\"\"Raised when no plan period is found\"\"\"\n\nPuhuri Entity Layout\n\n\n\nPuhuri-qal9000-entity-layout",
    "crumbs": [
      "Tutorials",
      "Puhuri-Tergite playground"
    ]
  },
  {
    "objectID": "tutorials/07_puhuri_tergite_flows.html#database-schemas",
    "href": "tutorials/07_puhuri_tergite_flows.html#database-schemas",
    "title": "Puhuri-Tergite playground",
    "section": "Database Schemas",
    "text": "Database Schemas\nThere are a few database schemas we may need. We are using mongodb here, but any kind of storage can be used/\nclass ProjectSource(str, enum.Enum):\n    PUHURI = 'puhuri'\n    INTERNAL = 'internal'\n\nclass Project(pydantic.BaseModel):\n    # ...\n    ext_id: str  # the project_uuid in this case\n    source: ProjectSource\n    user_emails: List[str] = []\n    qpu_seconds: int = 0\n    is_active: bool = True\n    resource_ids: List[str] = []",
    "crumbs": [
      "Tutorials",
      "Puhuri-Tergite playground"
    ]
  },
  {
    "objectID": "tutorials/07_puhuri_tergite_flows.html#operations",
    "href": "tutorials/07_puhuri_tergite_flows.html#operations",
    "title": "Puhuri-Tergite playground",
    "section": "Operations",
    "text": "Operations\nThere are some important operations we need to pull off. They include:\n\nReport usage on a per-project basis for projects that have Tergite offerings\nRetrieve latest approved resource allocations that have Tergite offerings from puhuri\nRetrieve latest users added to given projects that have Tergite offerings from puhuri\n\n\nProject-based Usage Report Submission\nThe flow of logic is as shown below\n\n\n\nPuhuri-qal9000-report-usage\n\n\n\nRetrieving Resources of a Given Project\nWe should be able to retrieve all resources attached to a project by running the cell below.\nNote that we need to first approve all pending orders for this provider. This ensures that all resources that we will query later have ‘plan periods’. (We should have updated our projects lists first)\nOnly resources with approved orders have plan periods.\nResources associated with approved orders have state “OK”. This is something we will filter for later.\n# Set the UUID of the project whose resources you wish to inspect\nPROJECT_UUID = \"\"\n\n_resource_filter = {\"provider_uuid\": PROVIDER_UUID, \"state\": \"OK\"}\nif PROJECT_UUID:\n    _resource_filter[\"project_uuid\"] = PROJECT_UUID\n\n# If you don't set the PROJECT_UUID, all resources that have offerings from\n# your given service provider will appear here\n_RESOURCES = CLIENT.filter_marketplace_resources(_resource_filter)\n_RESOURCES\nLet us check if there are any resources and exit with an error if none are found\nif len(_RESOURCES) == 0:\n    raise ResourceNotFoundError(f\"no resource found for provider and project\")\n\n\nSeparate Limit-based From Usage-based Resources\nSince Tergite keeps track of only the project uuid, and yet a project can have multiple resources, we need to determine the resource against which our usage report is to be made.\nCurrently, we think we should consume limit-based resources first before we move on to the usage-based resources.\nLimit-based resources are those that are prepaid i.e. can only be used after a given amount of QPU minutes has been purchased.\nOn the other hand, usage-based resources are billed, say at the end of the month.\nWe therefore need to separate limit-based resources of a project from usage-based resources and only report usage on the usage-based resources if there is no limit-based resource.\nQuestion: What should we do if all limit-based resources are depleted yet there are some usage-based resources? (Probably report on the usage-based resources)\n_USAGE_BASED_RESOURCES = []\n_LIMIT_BASED_RESOURCES = []\n\nfor resource in _RESOURCES:\n    # usage-based resources have an empty {} as their limits\n    if len(resource[\"limits\"]) == 0:\n        _USAGE_BASED_RESOURCES.append(resource)\n    else:\n        _LIMIT_BASED_RESOURCES.append(resource)\n\nprint(\"USAGE BASED RESOURCES\")\npprint.pprint(_USAGE_BASED_RESOURCES)\n\nprint(\"LIMIT BASED RESOURCES\")\npprint.pprint(_LIMIT_BASED_RESOURCES)\n\n\nSelecting the Right Resource to Report Usage Against\nWe are going to look through the different resources and select the right resource to report usage against.\n# the QPU seconds to be reported against the selected resource\n_QPU_SECONDS_USED = 80\n\n# the resource whose usage is to be updated\n_SELECTED_RESOURCE: Optional[Dict[str, Any]] = None\n\n# the accounting component to use when send resource usage.\n# Note: project -&gt; many resources -&gt; each with an (accounting) plan -&gt; each with multiple (accounting) components\n_SELECTED_COMPONENT: Optional[Dict[str, Any]] = None\n\n# the limit-based resources have a dictionary of \"limits\" with keys as the \"internal names\" or \"types\" of the components\n# and the values as the maximum amount for that component. This amount is in units of that component\n# e.g. 10 for one component, might mean 10 days, while for another it might mean 10 minutes depending\n# on the 'measurement_unit' of that component.\n# We will select the component whose limit (in seconds) &gt;= the usage\n_SELECTED_COMPONENT_TYPE: Optional[str] = None\nNOTE: We are making a big assumption that when creating components in the puhuri UI, the ‘measurement unit’s set on the component are of the following possible values: ’second’, ‘hour’, ‘minute’, ‘day’, ‘week’, ‘half_month’, and ‘month’.\nWe attempt to get the first limit-based resource that has a limit value (in seconds) greater or equal to the _QPU_SECONDS_USED to be reported. This is only polite to the customer so that we don’t run one resource to below zero while the others are one way above zero.\ndef get_accounting_component(\n        offering_uuid: str, \n        component_type: str, \n        cache: Optional[Dict[Tuple[str, str], Dict[str, Any]]] = None,\n        ) -&gt; Dict[str, Any]:\n    \"\"\"Gets the accounting component given the component type and the offering_uuid\n    \n    If the caches are provided, it attempts to extract the component \n    from the cache if the cache is provided\n\n    Args:\n        offering_uuid: the UUID string of the offering the component belongs to\n        component_type: the type of the component\n        cache: the dictionary cache that holds components, \n            accessible by (offering_uuid, component_type) tuple\n\n    Returns:\n        the component\n    \"\"\"\n    _cache = cache if isinstance(cache, dict) else {}\n    component = _cache.get((offering_uuid, component_type))\n\n    if component is None:\n        offering = CLIENT.get_marketplace_provider_offering(offering_uuid)\n        _cache.update({(offering_uuid, v[\"type\"]): v for v in offering[\"components\"]})\n        component = _cache[(offering_uuid, component_type)]\n\n    return component\n    \n\n# A map to help convert limits and amounts to-and-fro seconds given a particular accounting component\n_COMPONENT_UNIT_SECONDS_MAP: Dict[str, int] = {\n    \"month\": 30 * 24 * 3_600,\n    \"half_month\": 15 * 24 * 3_600,\n    \"week\": 7 * 24 * 3_600,\n    \"day\": 24 * 3_600,\n    \"hour\": 3_600,\n    \"minute\": 60,\n    \"second\": 1,\n}\n\n_COMPONENTS_CACHE: Dict[Tuple[str, str], Dict[str, Any]] = {}\n\nfor resource in _LIMIT_BASED_RESOURCES:\n    offering_uuid = resource[\"offering_uuid\"]\n\n    for comp_type, comp_amount in resource[\"limits\"].items():\n        component = get_accounting_component(\n            offering_uuid=offering_uuid, component_type=comp_type, cache=_COMPONENTS_CACHE)\n\n        unit_value = _COMPONENT_UNIT_SECONDS_MAP[component[\"measured_unit\"]]\n        limit_in_seconds = comp_amount * unit_value\n\n        # select resource which has at least one limit (or purchased QPU seconds) \n        # greater or equal to the seconds to be reported.\n        if limit_in_seconds &gt;= _QPU_SECONDS_USED:\n            _SELECTED_RESOURCE = resource\n            _SELECTED_COMPONENT = component\n            _SELECTED_COMPONENT_TYPE = comp_type\n            break\n\n    # get out of loop once we have a selected resource\n    if _SELECTED_RESOURCE is not None:\n        break\n\nprint(\"_SELECTED_RESOURCE\")\npprint.pprint(_SELECTED_RESOURCE)\n\nprint(\"_SELECTED_COMPONENT_TYPE\")\npprint.pprint(_SELECTED_COMPONENT_TYPE)\n\nprint(\"_SELECTED_COMPONENT\")\npprint.pprint(_SELECTED_COMPONENT)\nIf no limit-based resource has enough QPU minutes, we select the first usage-based resource. If no usage-based resource exists, we select the first limit-based resource.\nOf course if there are no resources at all, we should have not reached this far! We should have exited, with an error already.\nif _SELECTED_RESOURCE is None:\n    try:\n        _SELECTED_RESOURCE = _USAGE_BASED_RESOURCES[0]\n    except IndexError:\n        _SELECTED_RESOURCE = _LIMIT_BASED_RESOURCES[0]\n\n_SELECTED_RESOURCE\n\n\nGetting the Right Component Type\nWe need to get the corresponding accounting component type to use to report usage. If we got a limit-based resource, we should have already set the _SELECTED_COMPONENT_TYPE basing on the key in the limits dict that had an amount greater or equal to the QPU minutes we are going to report.\nRemember that limits is a dict containing the component types and their corresponding limits\nIf _SELECTED_COMPONENT_TYPE is not yet set, we need to obtain the first component type in the offering associated with the selected resource.\nLet us first get the offering that is associcated with the selected resource\n# This should not be necessary if you already have _SELECTED_COMPONENT_TYPE set\nif _SELECTED_COMPONENT_TYPE is None:\n    _SELECTED_OFFERING = CLIENT.get_marketplace_provider_offering(_SELECTED_RESOURCE[\"offering_uuid\"])\n\n    _SELECTED_OFFERING\nIf offering has no components, we raise an exception and exit\n# This should not be necessary if you already have _SELECTED_COMPONENT_TYPE set\nif _SELECTED_COMPONENT_TYPE is None:\n    _components = _SELECTED_OFFERING[\"components\"]\n    if len(_components) == 0:\n        raise ComponentNotFoundError(\"no components found for the selected offering\")\n    \n    _SELECTED_COMPONENT = _components[0]\n    _SELECTED_COMPONENT_TYPE = _SELECTED_COMPONENT[\"type\"]\n\n\nGenerate a Usage Report\nWe now need to generate the usage report to send over to puhuri\nLet us create a function to convert the QPU seconds into the component unit e.g “hour”, “month” e.t.c\ndef to_measured_unit(qpu_seconds: float, measured_unit: str) -&gt; float:\n    \"\"\"Converts the qpu seconds into the given measured unit of the component\n     \n    measured_unit e.g. hour, day etc\n    \n    Args:\n        qpu_seconds: the QPU seconds to convert\n        measured_unit: the 'measured_unit' of the accounting component\n\n    Returns:\n        the QPU time in 'measured_unit's\n    \"\"\"\n    # round up to two decimal places\n    return round(qpu_seconds / _COMPONENT_UNIT_SECONDS_MAP[measured_unit], 2)\nAnd then the usage report.\n\nGet the Plan Period for the Selected Resource\nIn order to send the usage report, one must sent the plan preiod UUID for the given resource. Note that only resources whose orders have been approved (or ar in state ‘OK’), have associated planned periods.\nLet’s retrieve the plan periods for the selected resource\n_PLAN_PERIODS = CLIENT.marketplace_resource_get_plan_periods(resource_uuid=_SELECTED_RESOURCE[\"uuid\"])\n\n_PLAN_PERIODS\nWe now need to get the plan period for the current month. We may not be sure whether there is only one plan period for this month or not so we will get the last one in the list for this month.\nLet’s first create some datetime utility functions\ndef to_datetime(timestamp: str) -&gt; datetime:\n    \"\"\"converts a timestamp of format like 2024-01-10T14:32:05.880079Z to datetime\n    \n    Args:\n        timestamp: the timestamp string\n\n    Returns:\n        the datetime corresponding to the given timestamp string\n    \"\"\"\n    return datetime.fromisoformat(timestamp.replace(\"Z\", \"+00:00\"))\n\n\ndef is_in_month(\n    month_year: Tuple[int, int],\n    timestamp: str,\n) -&gt; bool:\n    \"\"\"Checks if the given timestamp is in the given month\n\n    Note that months start at 1 i.e. January = 1, February = 2, ...\n\n    Args:\n        month_year: the (month, year) pair\n        timestamp: the timestamp string in format like 2024-01-10T14:32:05.880079Z\n\n    Returns:\n        True if the timestamp belongs to the same month, False if otherwise\n    \"\"\"\n    timestamp_date = to_datetime(timestamp)\n    month, year = month_year\n    return timestamp_date.month == month and timestamp_date.year == year\nNow let’s get the plan periods for the current month\n_NOW = datetime.now(tz=timezone.utc)\n_SELECTED_PLAN_PERIOD: Optional[Dict[str, Any]] = None\n\n_PLAN_PERIODS_FOR_CURRENT_MONTH = [\n    v for v in _PLAN_PERIODS \n    if is_in_month((_NOW.month, _NOW.year), v[\"start\"])\n]\n\ntry:\n    _SELECTED_PLAN_PERIOD = _PLAN_PERIODS_FOR_CURRENT_MONTH[-1]\nexcept IndexError:\n    raise PlanPeriodNotFoundError(f\"no plan period was found for month: {(_NOW.month, _NOW.year)}, for resource {_SELECTED_RESOURCE['uuid']}\")\n\n_SELECTED_PLAN_PERIOD\n\n\nSubmit the Usage Report\nWe are now ready to submit the usage report.\nWe will create a component usage for the first plan period of that given resource.\n# requests.post(_USAGE_REPORT_URL, data=_USAGE_REPORT_PAYLOAD, headers=_USAGE_REPORT_HEADERS)\nusage = ComponentUsage(\n    type= _SELECTED_COMPONENT_TYPE,\n    amount=to_measured_unit(\n    800, measured_unit=_SELECTED_COMPONENT[\"measured_unit\"]),\n    description= f\"{_QPU_SECONDS_USED} QPU seconds\",\n)\n\nCLIENT.create_component_usages(plan_period_uuid=_SELECTED_PLAN_PERIOD[\"uuid\"], \n                               usages=[usage])\n\n\n\nPuhuri Waldur Constraint: 1 Usage Per Month\nWhen we try again to submit another usage for the same plan, we may see that no new usage item is added to the ‘components’ of the plan period. To see this, you will have to run the previous code cell, the run the code cell before that.\nAs of now it seems like Puhuri can only accept 1 usage per month. This might change in future but for now, we must accumulate usages internally, and send the accumulated data\nThis means that only one request is expected per day. Any requests sent within that month (say 2024-01) will overwrite the previous entry for that month.\n\nHow to Work With Constraint\nWe will have to log every usage internally every month as a separate usage. We expect only one usage report for each job ID. Now and again, at whatever interval we wish, we will compute the accumulated usage for the current month and project and send it over to the Puhuri waldur server.\nWe do not have to wait for a month to elapse in order to resend usage reports for the same resource, (or plan) since it will be overwritten.\nFor us to limit access to users who have gone beyond the current limit for a given project, we might need to get the current pending QPUs left basing on the accumulated usage we have internally in Tergite, vs the allocated QPU seconds.",
    "crumbs": [
      "Tutorials",
      "Puhuri-Tergite playground"
    ]
  },
  {
    "objectID": "tutorials/07_puhuri_tergite_flows.html#getting-list-of-new-projects",
    "href": "tutorials/07_puhuri_tergite_flows.html#getting-list-of-new-projects",
    "title": "Puhuri-Tergite playground",
    "section": "Getting List of New Projects",
    "text": "Getting List of New Projects\nWe also need to get all new projects that have been created that are requesting for a resource governed by this service provider.\n\nGet All New Resources\nWe first get all the new resources attached to this provider uuid\n_NEW_RESOURCES = CLIENT.filter_marketplace_resources({\n    \"provider_uuid\": PROVIDER_UUID,\n    \"state\": \"Creating\"\n})\n\n_NEW_RESOURCES\nFrom these resources, we can extract the unique individual project UUID’s that are attached to these resources, and upsert them into our databases as new Project instances\nFirst of all we need to group the resources by project UUID.\nNote that a given pre-existing project can order new resources. When updating the database, we should upsert in such a way that we increment the prexisting qpu_seconds for any project that exists or we create a new project.\nLet’s group the new resources by project UUID. We will sum their “limit”s and “limit_usage”\nclass PuhuriProjectMetadata(pydantic.BaseModel):\n    \"\"\"Metadata as extracted from Puhuri resources\"\"\"\n    uuid: str \n    # dict of offering_uuid and limits dict\n    limits: Dict[str, Dict[str, float]] = {}\n    # dict of offering_uuid and limit_usage dict\n    limit_usage: Dict[str, Dict[str, float]] = {}\n    resource_uuids: List[str]\n\n\ndef remove_nones(data: Dict[str, Optional[Any]], __new: Any):\n    \"\"\"Replaces None values with the replacement\n    \n    Args:\n        data: the dictionary whose None values are to be replaced\n        __new: the replacement for the None values\n\n    Returns:\n        the dictionary with the None values replaced with the replacement\n    \"\"\"\n    return {k: v if v is not None else __new for k, v in data.items()}\n\n\ndef extract_project_metadata(resources: List[Dict[str, Any]]) -&gt; List[PuhuriProjectMetadata]:\n    \"\"\"Extracts the project metadata from a list of resources\n    \n    A project can contan any number of resources so we need to group the resources\n    by project UUID and aggregate any relevant fields like \"limits\" and \"limit_usage\"\n\n    Args:\n        resources: the list of resource dictionaries\n\n    Returns:\n        list of PuhuriProjectMetadata\n    \"\"\"\n    results: Dict[str, PuhuriProjectMetadata] = {}\n\n    for resource in resources:\n        offering_uuid = resource[\"offering_uuid\"]\n        project_uuid = resource[\"project_uuid\"]\n        limits = remove_nones(resource[\"limits\"], 0)\n        limit_usage = remove_nones(resource[\"limit_usage\"], 0)\n        project_meta = PuhuriProjectMetadata(\n                uuid=project_uuid,\n                limits={offering_uuid: limits},\n                limit_usage={offering_uuid: limit_usage},\n                resource_uuids=[resource[\"uuid\"]],\n            )\n        original_meta = results.get(project_uuid, None)\n        \n        if isinstance(original_meta, PuhuriProjectMetadata):\n            original_limits = original_meta.limits.get(offering_uuid, {})\n            project_meta.limits[offering_uuid] = {\n                k: v + original_limits.get(k, 0)\n                for k, v in limits.items() \n            }\n\n            original_usages = original_meta.limit_usage.get(offering_uuid, {})\n            project_meta.limit_usage[offering_uuid] = {\n                k: v + original_usages.get(k, 0)\n                for k, v in limit_usage.items() \n            }\n\n            project_meta.resource_uuids.extend(original_meta.resource_uuids)\n\n        results[project_uuid] = project_meta\n\n    return list(results.values())\n\n_NEW_PROJECT_METADATA = extract_project_metadata(_NEW_RESOURCES)\n\n_NEW_PROJECT_METADATA\n\nCompute the QPU seconds for each project\nBefore we can update our database, we need to compute the QPU seconds for each project.\nLet’s create a function to do just that\ndef get_qpu_seconds(metadata: PuhuriProjectMetadata) -&gt; float:\n    \"\"\"Computes the net QPU seconds the project is left with\n    \n    Args:\n        metadata: the metadata of the project\n\n    Returns:\n        the net QPU seconds, i.e. allocated minus used\n    \"\"\"\n    net_qpu_seconds = 0\n    _components_cache: Dict[Tuple[str, str], Dict[str, Any]] = {}\n\n    for offering_uuid, limits in metadata.limits.items():\n        limit_usage = metadata.limit_usage.get(offering_uuid, {})\n\n        for comp_type, comp_amount in limits.items():\n            component = get_accounting_component(\n                offering_uuid=offering_uuid, component_type=comp_type, cache=_components_cache)\n\n            unit_value = _COMPONENT_UNIT_SECONDS_MAP[component[\"measured_unit\"]]\n            net_comp_amount = comp_amount - limit_usage.get(comp_type, 0)\n            net_qpu_seconds += (net_comp_amount * unit_value)\n\n    return net_qpu_seconds\nWe can now extract Project instances from the PuhuriProjectMetadata\n_NEW_PROJECTS: List[Project] = [Project(\n    ext_id=item.uuid,\n    source=ProjectSource.PUHURI,\n    qpu_seconds=get_qpu_seconds(item),\n    is_active=False,\n    resource_ids=item.resource_uuids,\n) for item in _NEW_PROJECT_METADATA]\n\n_NEW_PROJECTS\n\n\nUpsert the New Projects into Database\nWe now have to upsert the new projects into the database.\nRemember, if any of the projects already exist, we should increment their QPU seconds; and their is_active set to False momentarily.\nNotice that we have not yet approved the above orders. However, all these new projects are set to is_active=False, until their orders are approved. This is after they are inserted into the database.\nFirst, let us create a unique Index for the project ext_id in the projects database collection. This is to ensure that no project has more than one entry.\nSideNote: beanie ODM can be a good tool to use to create database models that have indexes\n# if using mongo db\nif DB_CLIENT:\n    _db: motor_asyncio.AsyncIOMotorDatabase = DB_CLIENT[DB_NAME]\n    _collection: motor_asyncio.AsyncIOMotorCollection = _db[PROJECTS_COLLECTION]\n\n    await _collection.create_index(\"ext_id\", unique=True)\nThen we will update our projects\n# if using mongo db\nif DB_CLIENT:\n    _db: motor_asyncio.AsyncIOMotorDatabase = DB_CLIENT[DB_NAME]\n    _collection: motor_asyncio.AsyncIOMotorCollection = _db[PROJECTS_COLLECTION]\n\n    _responses = await asyncio.gather(*(_collection.update_one({\n            \"ext_id\": project.ext_id, \n            # a guard to ensure projects whose order approvals keep\n            # failing do not have their qpu_seconds incremented indefinitely\n            # NOTE: this may fail with a Conflict Error if any of the resource_ids \n            #   already exists in the project. You might need to resolve this manually\n            \"resource_ids\": {\"$nin\": project.resource_ids},\n            }, {\n            \"$set\": {\n                \"source\": ProjectSource.PUHURI.value,\n                \"is_active\": project.is_active,\n            },\n            \"$inc\": {\n                \"qpu_seconds\": project.qpu_seconds,\n            },\n            \"$addToSet\": {\"resource_ids\": {\"$each\": project.resource_ids}}\n        }, upsert=True) for project in _NEW_PROJECTS), return_exceptions=True)\n    \n    _UPDATED_PROJECTS = [\n        _NEW_PROJECTS[index]\n        for index, resp in enumerate(_responses)\n        if not isinstance(resp, Exception)\n    ]\n    \n\n    pprint.pprint(_UPDATED_PROJECTS)\n\n\nApprove All Orders for the given resources\nWe will then approve all the orders for the new resources and when successful, we will activate their associated projects\nWe will first create a function to do the approvals\nasync def approve_pending_orders(client: WaldurClient, provider_uuid: str, **kwargs) -&gt; Dict[str, Any]:\n    \"\"\"Approves all orders for the given service provider that are in the 'pending-provider' state\n\n    Args:\n        client: the WaldurClient\n        provider_uuid: the UUID string of the service provider\n        kwargs: extra filters for filtering the orders\n\n    Returns:\n        dictionary of kwargs used to filter orders.\n\n    Raises:\n        WaldurClientException: error making request\n        ValueError: no order item found for filter {kwargs}\n    \"\"\"\n    # This function does not have to be asynchronous in this notebook\n    # but it needs to be asynchronous in web-servers so that it does not\n    # block other requests. Same thing for all other functions that make\n    # network calls.\n    loop = asyncio.get_event_loop()\n\n    filter_obj = {\n        'state': 'pending-provider',\n        'provider_uuid': provider_uuid,\n        **kwargs\n    }\n    order_items = await loop.run_in_executor(None, client.list_orders, filter_obj)\n    if len(order_items) == 0:\n        raise ValueError(f\"no order item found for filter {kwargs}\")\n    \n    await asyncio.gather(*(loop.run_in_executor(None, client.marketplace_order_approve_by_provider, order[\"uuid\"]) \n                      for order in order_items),)\n    return kwargs\nThen we can make the API calls to Puhuri top approve the pending orders\n# if using mongo db\nif DB_CLIENT:\n    _RESOURCE_UUID_PROJECT_UUID_MAP = {\n        resource_uuid: project.ext_id\n        for project in _UPDATED_PROJECTS\n        for resource_uuid in project.resource_ids\n    }\n    pprint.pprint(_RESOURCE_UUID_PROJECT_UUID_MAP)\n\n    # _results is a list of Dict[resource_uuid, str]\n    # Note that we are filtering by resource UUID, not project UUID, because there is a chance \n    # that a project could have added new resources while we were still processing the \n    # current ones\n    _responses = await asyncio.gather(*(\n        approve_pending_orders(CLIENT, PROVIDER_UUID, resource_uuid=resource_uuid)\n        for resource_uuid in _RESOURCE_UUID_PROJECT_UUID_MAP.keys()\n    ), return_exceptions=True)\n\n    _APPROVED_RESOURCE_UUID_MAPS = [resp for resp in _responses if isinstance(resp, dict)]\n\n\n    pprint.pprint(_APPROVED_RESOURCE_UUID_MAPS)\nWe then have to get all approved projects i.e. projects without a non-approved order\n# if using mongo db\nif DB_CLIENT:\n    _results_map = {item[\"resource_uuid\"]: True for item in _APPROVED_RESOURCE_UUID_MAPS}\n\n    # approved projects are those that have all their resources approved\n    _APPROVED_PROJECT_UUIDS = [\n        metadata.uuid for metadata in _NEW_PROJECT_METADATA\n        if all(_results_map.get(resource_uuid) for resource_uuid in metadata.resource_uuids)\n    ]\n\n    pprint.pprint(_APPROVED_PROJECT_UUIDS)\nThen we update the approved projects in the database by setting their is_active to True.\nNote that some projects might have been active before new orders were queried. But then due to a failing order approval, they suddenly become inactive. That order must be resolved before they are reactivated, on the next run.\nOtherwise, no new resources will be added to that project\n# if using mongo db\nif DB_CLIENT:\n    _db: motor_asyncio.AsyncIOMotorDatabase = DB_CLIENT[DB_NAME]\n    _collection: motor_asyncio.AsyncIOMotorCollection = _db[PROJECTS_COLLECTION]\n\n    # set the approved projects' \"is_active\" to True\n    _result = await _collection.bulk_write(\n        [UpdateOne({\n            \"ext_id\": ext_id, \n            }, {\n            \"$set\": {\n                \"is_active\": True,\n            }\n        }) \n         for ext_id in _APPROVED_PROJECT_UUIDS]\n    )\n\n    pprint.pprint(_result)",
    "crumbs": [
      "Tutorials",
      "Puhuri-Tergite playground"
    ]
  },
  {
    "objectID": "tutorials/07_puhuri_tergite_flows.html#updating-qpu-seconds-for-each-pre-existing-project",
    "href": "tutorials/07_puhuri_tergite_flows.html#updating-qpu-seconds-for-each-pre-existing-project",
    "title": "Puhuri-Tergite playground",
    "section": "Updating QPU Seconds for Each Pre-existing Project",
    "text": "Updating QPU Seconds for Each Pre-existing Project\nWe need to update the allocated QPU seconds for all projects. This is like taking a snapshot of Puhuri’s state at a given time and transfering it to QAL9000.\nWe should first of all get all approved resources. This should ideally be done after approving all pending orders (i.e. after updating the project list in Tergite).\n_APPROVED_RESOURCES = CLIENT.filter_marketplace_resources({\n    \"provider_uuid\": PROVIDER_UUID,\n    \"state\": \"OK\"\n})\n\n_APPROVED_RESOURCES\nWe are assuming that all projects allocated to QAL9000 in puhuri will be “limit-based”, as this allows us to restrict usage of the quantum computer once the limits for the allocation have been exhausted.\nHowever, we are not able to stop any one from creating a ‘usage-based’ resource and attaching it to QAL9000. We will thus keep the qpu_seconds of such projects at 0 hence no access to their members.\nIn future, this could change. For instance, we could begin tracking whether a project is ‘usage-based’ or ‘limit-based’ within QAL9000. And then update the authorization logic to allow users whose projects are ‘usage-based’ even when their qpu_seconds is &lt;= 0\nLet us now add up all the net qpu_seconds for each project (since a project can have multiple resources).\nWe do this by grouping the resources into PuhuriProjectMetadata.\n_ALL_PROJECT_METADATA = extract_project_metadata(_APPROVED_RESOURCES)\n\n_ALL_PROJECT_METADATA\nThen we compute the QPU seconds of each project from the PuhuriProjectMetadata\n_ALL_PROJECTS: List[Project] = [Project(\n    ext_id=item.uuid,\n    source=ProjectSource.PUHURI,\n    qpu_seconds=get_qpu_seconds(item),\n    is_active=True,\n    resource_ids=item.resource_uuids,\n) for item in _ALL_PROJECT_METADATA]\n\n_ALL_PROJECTS\nWe then update the database; replacing the QPU seconds for each project with the new one.\n# if using mongo db\nif DB_CLIENT:\n    _db: motor_asyncio.AsyncIOMotorDatabase = DB_CLIENT[DB_NAME]\n    _collection: motor_asyncio.AsyncIOMotorCollection = _db[PROJECTS_COLLECTION]\n\n    _responses = await asyncio.gather(*(_collection.update_one({\n            \"ext_id\": project.ext_id, \n            # a guard to ensure that no resource is ignored\n            # NOTE: this may fail with a Conflict Error if any of the resource_ids \n            #   does not already exist in the project. You might need to resolve this manually\n            \"resource_ids\": {\"$in\": project.resource_ids},\n            }, \n            {\n                \"$set\": {\n                    \"source\": ProjectSource.PUHURI.value,\n                    \"is_active\": project.is_active,\n                    \"qpu_seconds\": project.qpu_seconds,\n                    \"resource_ids\": project.resource_ids,\n                },\n            }, \n        upsert=True) for project in _ALL_PROJECTS), return_exceptions=True)\n    \n    _ALL_UPDATED_PROJECTS = [\n        _ALL_PROJECTS[index]\n        for index, resp in enumerate(_responses)\n        if not isinstance(resp, Exception)\n    ]\n    \n\n    pprint.pprint(_ALL_UPDATED_PROJECTS)\n\nEnsuring Idempotency\nWe do not upsert in this case.\nAny projects whose approved resources have changed between the last time the database was updated with new resource_uuids and now, will not be updated by this routine.\nIt should be updated by the routine that deals with new resource allocations.",
    "crumbs": [
      "Tutorials",
      "Puhuri-Tergite playground"
    ]
  },
  {
    "objectID": "tutorials/07_puhuri_tergite_flows.html#updating-list-of-users-attached-to-projects",
    "href": "tutorials/07_puhuri_tergite_flows.html#updating-list-of-users-attached-to-projects",
    "title": "Puhuri-Tergite playground",
    "section": "Updating List of Users Attached to Projects",
    "text": "Updating List of Users Attached to Projects\nWe also need to be able to get all users that are all given projects recently.\nPreferably, it would be better to get any new user attachments as opposed to scanning all projects every single time. However, this seems like it is not possible at this time.\n\nGet All Approved Resources\nSo we need to first get all approved resources. This should ideally be done after approving all pending orders (i.e. after we have updated the project list in Tergite with the new orders).\n\n_APPROVED_RESOURCES = CLIENT.filter_marketplace_resources({\n    \"provider_uuid\": PROVIDER_UUID,\n    \"state\": \"OK\"\n})\n\n_APPROVED_RESOURCES\n\n\nGet the Teams for Each Approved Resource\nFor each approved resource, we extract the teams and associate them with the project_uuid for that resource\n# A map of project_uuid and the list of users attached to it\n_PROJECTS_USERS_MAP = {\n    resource[\"project_uuid\"]: CLIENT.marketplace_resource_get_team(resource[\"uuid\"])\n    for resource in _APPROVED_RESOURCES\n}\n\n_PROJECTS_USERS_MAP\nWe can them update our databases with the new projects users, ensuring to overwrite any existing user lists for any project, while closing all puhuri-attached projects that we have at we have in QAL9000 that have no users anymore.\nThis requires that we tag every project in QAL9000 with a ‘source’ attribute so that we are able to extract those that are from Puhuri (i.e. with source=‘puhuri’).\n# if using mongo db\nif DB_CLIENT:\n    _db: motor_asyncio.AsyncIOMotorDatabase = DB_CLIENT[DB_NAME]\n    _collection: motor_asyncio.AsyncIOMotorCollection = _db[PROJECTS_COLLECTION]\n\n    await _collection.bulk_write(\n        [UpdateOne({\n            \"ext_id\": project_id, \n            \"source\": ProjectSource.PUHURI.value,\n            }, {\n            \"$set\": {\n                \"user_emails\": [user[\"email\"] for user in user_list],\n            }\n        }) \n         for project_id, user_list in _PROJECTS_USERS_MAP.items()]\n    )\n\n\nEnsuring Idempotency\nWe do not upsert in this case. We just replace the user_emails field with the new lists\nThis also requires that we bulk update all projects whose ‘external ID’ are not IN the list of keys from _PROJECTS_USERS_MAP and yet have ‘source’ = ‘puhuri’\n# if using mongo db\nif DB_CLIENT:\n    _db: motor_asyncio.AsyncIOMotorDatabase = DB_CLIENT[DB_NAME]\n    _collection: motor_asyncio.AsyncIOMotorCollection = _db[PROJECTS_COLLECTION]\n\n    \n    _filter_obj = {\n        \"source\": ProjectSource.PUHURI.value, \n        \"ext_id\": {\"$nin\": list(_PROJECTS_USERS_MAP.keys())},\n    }\n    await _collection.update_many(filter=_filter_obj, update={\n        \"$set\": {\"user_emails\": []}\n    })",
    "crumbs": [
      "Tutorials",
      "Puhuri-Tergite playground"
    ]
  },
  {
    "objectID": "tutorials/05_hardware_configuration.html",
    "href": "tutorials/05_hardware_configuration.html",
    "title": "Hardware Configuration",
    "section": "",
    "text": "Documentation about configuring tergite-backend",
    "crumbs": [
      "Tutorials",
      "Hardware Configuration"
    ]
  },
  {
    "objectID": "tutorials/05_hardware_configuration.html#general-configuration",
    "href": "tutorials/05_hardware_configuration.html#general-configuration",
    "title": "Hardware Configuration",
    "section": "General Configuration",
    "text": "General Configuration\nTo configure the entire tergite-backend application, we use .env files.\nJust copy the dot-env-template.txt to env and update the variables there in.\ncp dot-env-template.txt .env",
    "crumbs": [
      "Tutorials",
      "Hardware Configuration"
    ]
  },
  {
    "objectID": "tutorials/05_hardware_configuration.html#qblox-instruments-configuration",
    "href": "tutorials/05_hardware_configuration.html#qblox-instruments-configuration",
    "title": "Hardware Configuration",
    "section": "QBLOX Instruments Configuration",
    "text": "QBLOX Instruments Configuration\nWe use the quantify-config.example.json as a template for how to configure this application to control the QBLOX instruments that control the quantum computer.\nThis configuration file uses QBLOX Harware Compilation Configuration format, which can be found here.\nPlease check the documentation on how to properly create configuration for QBLOX instruments: Quantify documentation\nCopy this template for a minimal example configuration with one control module and one readout module connected to microwave and resonator ports appropriately.\ncp quantify-config.example.json quantify-config.json\n\nQBLOX Metadata configuration\nIn addition to QBLOX Hardware Compilation Configuration, you need to provide a metadata for your instrument setup. This is a small template file with a simple structure that is used to describe clusters names, ip_addresses, whether this is a dummy cluster. In addition it is also used to provide a QBLOX module dictionary for a dummy clusters.\nHowever this configuration file has a redundancy with the QBLOX Hardware Compilation Confiugration and might be changed in the future releases.\nWe use quantify-metadata.yml as a template for providing metadata.\nCopy this documented template and you can fill in your cluster’s ip_address and desired configuration.\ncp quantify-metadata.example.yml quantify-config.yml\n\n\nDummy QBLOX Instrumments\nYou may wish to run some dummy QBLOX instruments if you don’t have access to the physical QBLOX instruments\nYou may leave QBLOX Hardware Compilation Configuration the same as a real configuration, and change is_dummy field to True for your dummy cluster in the quantify-metadata.yml\nThen you may also need to provide a module dictionary for your dummy cluster in order to successfully compile your dummy schedule.\nWe already have a preconfigured dummy-quantify-metadata.yml for this in the app/tests/fixtures folder.\nAs well as corresponding dummy-quantify-config.json for this in the app/tests/fixtures folder.\nCopy it to the tergite-backend folder.\ncp app/tests/fixtures/dummy-quantify-metadata.yml quantify-metadata.yml\nAs well as quantify-config.yml\ncp app/tests/fixtures/dummy-quantify-config.json quantify-config.json\nNOTE: You can find out more about the configuration properties in the executor-config file by visiting the quantify_scheduler docs and the QCoDeS drivers docs.\nNOTE: You could choose to use a different name for your quantum executor config file e.g. foobar.json. You however need to explicitly set this name in the .env file QUANTIFY_CONFIG_FILE=foobar.json\nNOTE: The same works for quantify metadata file e.g. foobar.yml. You also need to explicitly set this name in the .env file QUANTIFY_METADATA_FILE=foobar.yml\n\n\nGeneral Backend Configuration\nWe configure all backends using the backend_config.toml.\nWe use the backend_config.example.toml as a template.\n\n\nBackend Configuration Calibrations\nWhenever you need to pass calibration values to your backend for example for a pulse-level simulator or a dummy QBLOX backend. You need to also create a calibration.seed.toml file with the pre-configured values. You don’t have to provide this file for a real backend.\nWe use the calibration.seed.example.toml as a template.\nNOTE: You don’t need to pass the .env file, the backend_config.toml file or the quantify-config.json file as well as calibration.seed.toml and quantify-metadata.yml to the start script as these are automatically loaded for you.\n\nSingle-Qubit Qiskit Pulse Simulator\nYou may wish to run a single-qubit simulator.\nFirst update the .env file to contain EXECUTOR_TYPE=qiskit_pulse_1q.\nWe already have a preconfigured backend_config.simq1.toml for this in the app/tests/fixtures folder.\nAs well as a preconfigured calibrations qiskit_pulse_1q.seed.toml for this in the app/tests/fixtures folder.\nCopy it to your root folder.\n# on the root of the project\ncp app/tests/fixtures/backend_config.simq1.toml backend_config.toml\n# on the root of the project\ncp app/tests/fixtures/qiskit_pulse_1q.seed.toml calibration.seed.toml\nAnd run the application.\n./start_bcc.sh",
    "crumbs": [
      "Tutorials",
      "Hardware Configuration"
    ]
  },
  {
    "objectID": "tutorials/03_authentication.html",
    "href": "tutorials/03_authentication.html",
    "title": "Authentication",
    "section": "",
    "text": "This is how the Main Service Server (MSS) in tergite-frontend authenticates its users.",
    "crumbs": [
      "Tutorials",
      "Authentication"
    ]
  },
  {
    "objectID": "tutorials/03_authentication.html#add-a-new-oauth2-provider",
    "href": "tutorials/03_authentication.html#add-a-new-oauth2-provider",
    "title": "Authentication",
    "section": "Add a New Oauth2 Provider",
    "text": "Add a New Oauth2 Provider\n\nUpdate tergite-mss app in tergite-frontend\n\nLet’s say we want some ‘Company B’ users to have access to MSS.\nCopy the mss-config.example.toml to mss-config.toml in the tergite-frontend folder, and update the configs therein.\nNote: You could also create a new toml file based on mss-config.example.toml\nand set the MSS_CONFIG_FILE environment variable to point to that file.\nAdd the new client:\n\n[[auth.clients]]\n# this name will appear in the URLs e.g. http://127.0.0.1:8002/auth/company-b/...\nname = \"company-b\"\nclient_id = \"some-openid-client-id\"\nclient_secret = \"some-openid-client-secret\"\n# the URL to redirect to after user authenticates with the system.\n# It is of the format {MSS_BASE_URL}/auth/app/{provider_name}/callback\nredirect_url = \"http://127.0.0.1:8002/auth/company-b/callback\"\nclient_type = \"openid\"\nemail_regex = \".*\"\n# the domain of the emails of the users that should be directed to this auth client\nemail_domain = \"example.com\"\n# Roles that are automatically given to users who authenticate through Company B\n# roles can be: \"admin\", \"user\", \"researcher\", \"partner\". Default is \"user\".\nroles = [\"partner\", \"user\"]\n# openid_configuration_endpoint is necessary if Company B uses OpenID Connect, otherwise ignore.\nopenid_configuration_endpoint = \"https://proxy.acc.puhuri.eduteams.org/.well-known/openid-configuration\"\n\n\nUpdate tergite-landing-page app in tergite-frontend\n\nAdd the logo for the new provider in the tergite-frontend/apps/tergite-landing-page/public/img folder.\nOpen the tergite-frontend/apps/tergite-landing-page/src/app/api/config/route.ts file and update its OAUTH2_LOGOS list to include your new provider’s logo.\n\nconst OAUTH2_LOGOS: { [key: string]: string } = {\n    github: '/img/github-black.png',\n    chalmers: '/img/chalmers-logo.svg',\n    company-b: '/img/company-b-logo.svg',\n};\n\nStart the frontend application. Instructions are on the tergite-frontend/README.md\n\ndocker compose -f fresh-docker-compose.yml up -d\nNote: The tergite-landing-page and tergite-mss apps must share the same domain if they are to work with cookies.",
    "crumbs": [
      "Tutorials",
      "Authentication"
    ]
  },
  {
    "objectID": "tutorials/03_authentication.html#faqs",
    "href": "tutorials/03_authentication.html#faqs",
    "title": "Authentication",
    "section": "FAQs",
    "text": "FAQs\n\n- How do we bypass authentication in development?\nWe use feature flag auth.is_enabled property in the mss-config.toml file, setting it to false\nis_enabled = false\nNote: /auth endpoints will still require authentication because they depend on the current user\n\n\n- How do we ensure that in production, authentication is always turned on?\nOn startup, we raise a ValueError when auth.is_enabled = false in the mss-config.toml file yet\nconfig variable environment = production and log it.\n\n\n- How do we allow other qal9000 services (e.g. tergite-backend or calibration workers) to access MSS, without user intervention?\nUse app tokens created by any user who had the ‘system’ role. The advantage of using app tokens is that they are more secure because they can easily be revoked and scoped. Since they won’t be used to run jobs, their project QPU seconds are expected not to run out.\nIf you are in development mode, you can just switch of authentication altogether.\n\n\nHow do I log in?\n\nYou need to run the tergite-frontend.\nMake sure that your mss-config.toml files have all variables filled appropriately.\nThe landing page, when running, has appropriate links, say in the navbar, to direct you on how to the authentication screens.\n\n\n\nHow does the backend get authenticated?\n\nA client (say tergite) sends a POST request is sent to /jobs on MSS (this app) with an app_token in its Authorization header\nA new job entry is created in the database, together with a new unique job_id.\nMSS notifies tergite-backend of the job_id and its associated app_token by sending a POST request to /auth endpoint of tergite-backend.\nIn the response to the client, MSS returns the /jobs url for the given tergite-backend instance\nThe client then sends its experiment data to the tergite-backend /jobs url, with the same app_token in its Authorization header and the same job_id in the experiment data.\ntergite-backend checks if the job_id and the app_token are first of all associated, and if no other experiment data has been sent already with the same job_id-app_token pair. This is to ensure no user attempts to fool the system by using the same job_id for multiple experiments, which is theoretically possible.\nIf tergite-backend is comfortable with the results of the check, it allows the job to be submitted. Otherwise, either a 401 or a 403 HTTP error is thrown.\nThe same job_id-app_token pair is used to download raw logfiles from tergite-backend at /logfiles/{job_id} endpoint. This time, tergite-backend just checks that the pair match but it does not check if the pair was used already.\nThis is the same behaviour when reading the job results at jobs/{job_id}/result or the job status at jobs/{job_id}/status or the entire job entry at jobs/{job_id} in tergite-backend.\nThis is also the same behaviour when attempting to delete the job at /jobs/{job_id} or to cancel it at /jobs/{job_id}/cancel in tergite-backend.",
    "crumbs": [
      "Tutorials",
      "Authentication"
    ]
  },
  {
    "objectID": "tutorials/01_quick_start.html",
    "href": "tutorials/01_quick_start.html",
    "title": "Quick Start",
    "section": "",
    "text": "Let’s attempt to setup the Tergite stack to run on a simulator on your local machine.\nWe will not need an actual quantum computer.",
    "crumbs": [
      "Tutorials",
      "Quick Start"
    ]
  },
  {
    "objectID": "tutorials/01_quick_start.html#prerequisites",
    "href": "tutorials/01_quick_start.html#prerequisites",
    "title": "Quick Start",
    "section": "Prerequisites",
    "text": "Prerequisites\nYou may have to install these software if you don’t have them already installed.\n\nDocker +v23.0.5\nConda\nRedis\nMongoDb\nVisual Studio Code\nMongo compass",
    "crumbs": [
      "Tutorials",
      "Quick Start"
    ]
  },
  {
    "objectID": "tutorials/01_quick_start.html#setup-the-frontend",
    "href": "tutorials/01_quick_start.html#setup-the-frontend",
    "title": "Quick Start",
    "section": "Setup the Frontend",
    "text": "Setup the Frontend\n\nEnsure you have docker is running.\n\ndocker --help\nNote: for MacOS, start docker by running this command\nopen -a Docker\nNote: for Windows, start docker by running this command\nStart-Process \"C:\\Program Files\\Docker\\Docker\\Docker Desktop.exe\"\nNote: for Linux, start docker by running this command\nsudo systemctl start docker\n\nClone the tergite-frontend repo\n\ngit clone https://github.com/tergite/tergite-frontend.git\n\nEnter the tergite-frontend folder\n\ncd tergite-frontend\n\nCreate an mss-config.toml file with visual studio code (or any other text editor).\n\ncode mss-config.toml\n\nUpdate the mss-config.toml with the following content\n\n# mss-config.toml\n\n# general configurations\n[general]\n# the port on which MSS is running\nmss_port = 8002\n# the port on which the websocket is running\nws_port = 6532\n# environment reflect which environment the app is to run in.\nenvironment = \"development\"\n# the host the uvicorn runs on.\n# During testing auth on 127.0.0.1, set this to \"127.0.0.1\". default: \"0.0.0.0\"\nmss_host = \"127.0.0.1\"\n\n[database]\n# configurations for the database\nname = \"testing\"\n# database URI\n# host.docker.internal resolves to the host's 127.0.0.1\n# see https://stackoverflow.com/questions/31324981/how-to-access-host-port-from-docker-container#answer-43541732\nurl = \"mongodb://host.docker.internal:27017\"\n\n[[backends]]\nname = \"qiskit_pulse_2q\"\n# the URL where this backend is running\n# host.docker.internal resolves to the host's 127.0.0.1\n# see https://stackoverflow.com/questions/31324981/how-to-access-host-port-from-docker-container#answer-43541732\nurl = \"http://host.docker.internal:8000\"\n\n[auth]\n# turn auth OFF or ON, default=true\nis_enabled = false\ncookie_domain = \"127.0.0.1\"\ncookie_name = \"tergiteauth\"\n\n[[auth.clients]]\nname = \"github\"\nclient_id = \"some-github-obtained-client-id\"\nclient_secret = \"some-github-obtained-client-secret\"\nredirect_url = \"http://127.0.0.1:8002/auth/github/callback\"\nclient_type = \"github\"\nemail_regex = \"^(john\\\\.doe|jane|aggrey)@example\\\\.com$\"\nemail_domain = \"example.com\"\nroles = [\"admin\", \"user\"]\n\n[[auth.clients]]\nname = \"puhuri\"\nclient_id = \"some-puhuri-obtained-client-id\"\nclient_secret = \"some-puhuri-obtained-client-secret\"\nredirect_url = \"http://127.0.0.1:8002/auth/puhuri/callback\"\nclient_type = \"openid\"\nemail_regex = \"^(john\\\\.doe|jane)@example\\\\.com$\"\nemail_domain = \"example.com\"\nroles = [\"user\"]\nopenid_configuration_endpoint = \"https://proxy.acc.puhuri.eduteams.org/.well-known/openid-configuration\"\n\n# Puhuri synchronization\n# Puhuri is a resource management platform for HPC systems, that is also to be used for Quantum Computer's\n[puhuri]\n# turn puhuri synchronization OFF or ON, default=true\nis_enabled = false\n\nCreate a .env file with visual studio code (or any other text editor).\n\ncode .env\n\nUpdate the .env with the following content\n\n# .env\n\nMSS_PORT=8002\n\n# required\nENVIRONMENT=\"development\"\nMSS_URL=\"http://127.0.0.1:8002\"\nGRAFANA_LOKI_URL=http://127.0.0.1:3100/loki/api/v1/push\nLOKI_LOGGER_ID=some-generic-id\n\n# docker LOGGING_DRIVER can be journald, json-file, local etc. \nLOGGING_DRIVER=json-file\n# image versions:\n# Note: If you ever want the images to be rebuilt, \n# you have to change the app version numbers here \n# before running \"docker compose up\"\nMSS_VERSION=v0.0.1\nDASHBOARD_VERSION=v0.0.1\nPROMTAIL_VERSION=2.8.3\n\nFor Linux: open MongoDB configurations file\n\ncode /etc/mongod.conf\n\nFor Linux: Replace the contents that config file with the following:\n\n# mongod.conf\n\n# for documentation of all options, see:\n#   http://docs.mongodb.org/manual/reference/configuration-options/\n\n# Where and how to store data.\nstorage:\n  dbPath: /var/lib/mongodb\n#  engine:\n#  wiredTiger:\n\n# where to write logging data.\nsystemLog:\n  destination: file\n  logAppend: true\n  path: /var/log/mongodb/mongod.log\n\n# network interfaces\nnet:\n  port: 27017\n  bindIp: 0.0.0.0\n\n\n# how the process runs\nprocessManagement:\n  timeZoneInfo: /usr/share/zoneinfo\n\n#security:\n\n#operationProfiling:\n\n#replication:\n\n#sharding:\n\n## Enterprise-Only Options:\n\n#auditLog:\n\nFor Linux: restart mongod service and make sure that it’s active\n\nsudo service mongod restart\nsudo service mongod status\n\nOpen the Mongo compass application and connect to the default local mongo database\n\n\n\n\n\n\n\n\n\n\n\n\nCreate a new mongo database called “testing” that contains a “backends” collection.\n\n\n\n\n\n\n\n\n\n\n\n\nDelete the old docker images of “tergite/tergite-mss”, “tergite/tergite-dashboard” from docker if they exist.\n\ndocker rmi -f tergite/tergite-mss:v0.0.1\ndocker rmi -f tergite/tergite-dashboard:v0.0.1\n\nTo Run the services, use the fresh-docker-compose.yml.\n\ndocker compose -f fresh-docker-compose.yml up -d\n\nRemove any stale artefacts created during the docker build\n\ndocker system prune\n\nOpen your browser at\n\nhttp://localhost:8002 to see the MSS service\nhttp://localhost:3000 to see the Dashboard application\n\nTo view the status of the services, run:\n\ndocker compose -f fresh-docker-compose.yml ps\n\nTo stop the services, run:\n\ndocker compose -f fresh-docker-compose.yml stop\n\nTo remove stop the services and remove their containers also, run:\n\ndocker compose -f fresh-docker-compose.yml down\n\nTo view logs of the docker containers to catch some errors, use:\n\ndocker compose -f fresh-docker-compose.yml logs -f\nsee more at https://docs.docker.com/reference/cli/docker/compose/logs/\n\nEnsure that the services are running. If they are not, restart them.\n\ndocker compose -f fresh-docker-compose.yml up -d",
    "crumbs": [
      "Tutorials",
      "Quick Start"
    ]
  },
  {
    "objectID": "tutorials/01_quick_start.html#setup-the-backend",
    "href": "tutorials/01_quick_start.html#setup-the-backend",
    "title": "Quick Start",
    "section": "Setup the Backend",
    "text": "Setup the Backend\n\nEnsure you have conda installed. (You could simply have python +3.12 installed instead.)\nEnsure you have the Redis server running.\n\nredis-server\n\nOpen terminal.\nClone the tergite-backend repo\n\ngit clone https://github.com/tergite/tergite-backend.git\n\nCreate conda environment\n\nconda create -n bcc -y python=3.12\nconda activate bcc\n\nInstall dependencies\n\ncd tergite-backend\npip install .\n\nCreate an .env file with visual studio code (or any other text editor).\n\ncode .env\n\nUpdate the .env file with the following content.\n\nAPP_SETTINGS=development\nIS_AUTH_ENABLED=False\n\nDEFAULT_PREFIX=qiskit_pulse_2q\nSTORAGE_ROOT=/tmp\nLOGFILE_DOWNLOAD_POOL_DIRNAME=logfile_download_pool\nLOGFILE_UPLOAD_POOL_DIRNAME=logfile_upload_pool\nJOB_UPLOAD_POOL_DIRNAME=job_upload_pool\nJOB_PRE_PROC_POOL_DIRNAME=job_preproc_pool\nJOB_EXECUTION_POOL_DIRNAME=job_execution_pool\n\n# Main Service Server\nMSS_MACHINE_ROOT_URL=http://localhost:8002\nMSS_PORT=8002\n\n# Backend Control computer\nBCC_MACHINE_ROOT_URL=http://localhost:8000\nBCC_PORT=8000\n\nEXECUTOR_TYPE=qiskit_pulse_2q\n\nLOG_LEVEL=INFO\nDEBUG=false\nUVICORN_LOG_LEVEL=info\n\nCreate a backend_config.toml file with visual studio code (or any other text editor).\n\ncode backend_config.toml\n\nUpdate the backend_config.toml file with the following content.\n\n# backend_config.toml\n[general_config]\nname = \"qiskit_pulse_2q\"\nis_active = true\ncharacterized = true\nopen_pulse = true\nsimulator = true\nversion = \"1.0.0\"\nonline_date = \"2024-10-09T00:00:00\"\nnum_qubits = 2\nnum_couplers = 1\nnum_resonators = 2\ndescription = \"A two-qubit transmon Hamiltonian with 4 levels per qubit\"\ndt = 0.01e-9\ndtm = 0.01e-9\n\n[device_config]\ndiscriminators = [ \"lda\" ]\nqubit_ids = [ \"q0\", \"q1\" ]\nmeas_map = [ [ 0 ], [ 1 ] ]\ncoordinates = [\n  [0, 0],\n  [1, 0]\n]\n\nqubit_parameters = [\n  \"id\",\n  \"x_position\",\n  \"y_position\",\n  \"xy_drive_line\",\n  \"z_drive_line\",\n  \"frequency\",\n  \"pi_pulse_amplitude\",\n  \"pi_pulse_duration\",\n  \"pulse_type\",\n  \"pulse_sigma\",\n  \"t1_decoherence\",\n  \"t2_decoherence\"\n]\nresonator_parameters = [\n  \"id\",\n  \"x_position\",\n  \"y_position\",\n  \"readout_line\",\n  \"acq_delay\",\n  \"acq_integration_time\",\n  \"frequency\",\n  \"pulse_delay\",\n  \"pulse_duration\",\n  \"pulse_type\",\n  \"pulse_amplitude\"\n]\n\ncoupler_parameters = [\n  \"id\",\n  \"frequency\",\n  \"frequency_detuning\",\n  \"anharmonicity\",\n  \"coupling_strength_02\",\n  \"coupling_strength_12\",\n  \"cz_pulse_amplitude\",\n  \"cz_pulse_dc_bias\",\n  \"cz_pulse_phase_offset\",\n  \"cz_pulse_duration_before\",\n  \"cz_pulse_duration_rise\",\n  \"cz_pulse_duration_constant\",\n  \"pulse_type\"\n]\n\n[device_config.discriminator_parameters]\nlda = [\n  \"coef_0\",\n  \"coef_1\",\n  \"intercept\"\n]\n\n[device_config.coupling_dict]\nu0 = [\"q0\", \"q1\"]\n\n[gates.x]\ncoupling_map = [ [ 0, 1], [1, 0] ]\nqasm_def = \"gate x q { U(pi, 0, pi) q; }\"\nparameters = [ ]\n\nCreate a calibration.seed.toml file with visual studio code (or any other text editor).\n\ncode calibration.seed.toml\n\nUpdate the calibration.seed.toml file with the following content.\n\n[calibration_config]\ncoupling_strength = 0.02e9\n\n[calibration_config.units.qubit]\nfrequency = \"Hz\"\nt1_decoherence = \"s\"\nt2_decoherence = \"s\"\nanharmonicity = \"Hz\"\n\n[calibration_config.units.readout_resonator]\nacq_delay = \"s\"\nacq_integration_time = \"s\"\nfrequency = \"Hz\"\npulse_delay = \"s\"\npulse_duration = \"s\"\npulse_amplitude = \"\"\npulse_type = \"\"\n\n[calibration_config.units.coupler]\nfrequency = \"Hz\"\nfrequency_detuning = \"Hz\"\nanharmonicity = \"Hz\"\ncoupling_strength_02 = \"Hz\"\ncoupling_strength_12 = \"Hz\"\ncz_pulse_amplitude = \"\"\ncz_pulse_dc_bias = \"\"\ncz_pulse_phase_offset = \"rad\"\ncz_pulse_duration_before = \"s\"\ncz_pulse_duration_rise = \"s\"\ncz_pulse_duration_constant = \"s\"\npulse_type = \"\"\n\n[[calibration_config.qubit]]\nid = \"q0\"\nfrequency = 4.8e9\nanharmonicity = -0.17e9\nt1_decoherence = 3.4e-5\nt2_decoherence = 3.3e-5\npi_pulse_amplitude = 0.029\npi_pulse_duration = 5.6e-8\npulse_sigma = 7e-9\npulse_type = \"Gaussian\"\n\n[[calibration_config.qubit]]\nid = \"q1\"\nfrequency = 4.225e9\nanharmonicity = -0.17e9\nt1_decoherence = 3.4e-5\nt2_decoherence = 3.3e-5\npi_pulse_amplitude = 0.029\npi_pulse_duration = 5.6e-8\npulse_sigma = 7e-9\npulse_type = \"Gaussian\"\n\n[[calibration_config.readout_resonator]]\nid = \"q0\"\nacq_delay = 0\nacq_integration_time = 0\nfrequency = 0\npulse_delay = 0\npulse_duration = 0\npulse_amplitude = 0\npulse_type = \"\"\n\n[[calibration_config.readout_resonator]]\nid = \"q1\"\nacq_delay = 0\nacq_integration_time = 0\nfrequency = 0\npulse_delay = 0\npulse_duration = 0\npulse_amplitude = 0\npulse_type = \"\"\n\n[[calibration_config.coupler]]\nid = \"u0\"\nfrequency = 7.8e9\nfrequency_detuning = -0.015e9\nanharmonicity = -0.17e9\ncoupling_strength_02 = 0.07e9\ncoupling_strength_12 = 0.07e9\ncz_pulse_amplitude = 0.08\ncz_pulse_dc_bias = 0.275\ncz_pulse_phase_offset = 0\ncz_pulse_duration_before = 88e-9\ncz_pulse_duration_rise = 25e-9\ncz_pulse_duration_constant = 370e-9\npulse_type = \"wacqt_cz_gate_pulse\"\n\n[calibration_config.discriminators.lda.q0]\nintercept = -37.36457774060694\ncoef_0 = 22.99757458442274\ncoef_1 = -0.38509625914248247\n\n[calibration_config.discriminators.lda.q1]\nintercept = -37.36457774060694\ncoef_0 = 22.99757458442274\ncoef_1 = -0.38509625914248247\n\nRun start script\n\n./start_bcc.sh\n\nOpen your browser at http://localhost:8000/docs to see the interactive API docs",
    "crumbs": [
      "Tutorials",
      "Quick Start"
    ]
  },
  {
    "objectID": "tutorials/01_quick_start.html#run-an-experiment",
    "href": "tutorials/01_quick_start.html#run-an-experiment",
    "title": "Quick Start",
    "section": "Run an Experiment",
    "text": "Run an Experiment\n\nOpen another terminal\nCreate a new folder “tergite-test” and enter it\n\nmkdir tergite-test\ncd tergite-test\n\nCreate conda environment and activate it\n\nconda create -n tergite -y python=3.12\nconda activate tergite\n\nInstall qiskit and Tergite SDK by running the command below:\n\npip install qiskit\npip install tergite\n\nCreate a file main.py with visual studio code (or any other text editor).\n\ncode main.py\n\nUpdate the main.py file with the following content:\n\n# main.py\n\"\"\"A sample script doing a very simple quantum operation\"\"\"\nimport time\n\nimport qiskit.circuit as circuit\nimport qiskit.compiler as compiler\n\nfrom tergite import Job, Tergite\n\nif __name__ == \"__main__\":\n    # the Tergite API URL\n    API_URL = \"http://localhost:8002\"\n    # The name of the Quantum Computer to use from the available quantum computers\n    BACKEND_NAME = \"qiskit_pulse_2q\"\n    # the name of this service. For your own bookkeeping.\n    SERVICE_NAME = \"local\"\n    # the timeout in seconds for how long to keep checking for results\n    POLL_TIMEOUT = 100\n\n    # create the Qiskit circuit\n    qc = circuit.QuantumCircuit(2, 2)\n    qc.h(0)\n    qc.cx(0, 1)\n    qc.measure(0, 0)\n    qc.measure(1, 1)\n\n    # create a provider\n    # provider account creation can be skipped in case you already saved\n    # your provider account to the `~/.qiskit/tergiterc` file.\n    # See below how that is done.\n    provider = Tergite.use_provider_account(service_name=SERVICE_NAME, url=API_URL)\n    # to save this account to the `~/.qiskit/tergiterc` file, add the `save=True`\n    # provider = Tergite.use_provider_account(service_name=SERVICE_NAME, url=API_URL, save=True)\n\n    # Get the Tergite backend in case you skipped provider account creation\n    # provider = Tergite.get_provider(service_name=SERVICE_NAME)\n    backend = provider.get_backend(BACKEND_NAME)\n    backend.set_options(shots=1024)\n\n    # compile the circuit\n    tc = compiler.transpile(qc, backend=backend)\n\n    # run the circuit\n    job: Job = backend.run(tc, meas_level=2, meas_return=\"single\")\n    job.wait_for_final_state(timeout=POLL_TIMEOUT)\n\n    # view the results\n    result = job.result()\n    print(result.get_counts())\n\nExecute the above script by running the command below.\n\npython main.py\n\nIt should return something like:\n\nResults OK\n{'00': 495, '01': 51, '11': 449, '10': 29}",
    "crumbs": [
      "Tutorials",
      "Quick Start"
    ]
  },
  {
    "objectID": "release_notes/tergite-frontend.html#section",
    "href": "release_notes/tergite-frontend.html#section",
    "title": "Tergite Frontend",
    "section": "[2025.09.0]",
    "text": "[2025.09.0]\nreleased on: 2nd of October 2025\n\nAdded\n\nAdd external loki logger support for promtail for linux\nMSS\n\nAdded access_token field to the job schema. It is encrypted when in database but plain when requested for by anyone who has access to the job (including other members of the project).\n\n\n\n\nChanged\n\nMSS\n\nBREAKING: Changed the response to job creation /jobs POST endpoint, to return a JWT access token also\nBREAKING: Changed the authentication with BCC to use private key signed signatures in headers\nBREAKING: Removed the option to disable authentication as it is a requirement for BCC",
    "crumbs": [
      "Release Notes",
      "Tergite Frontend"
    ]
  },
  {
    "objectID": "release_notes/tergite-frontend.html#section-1",
    "href": "release_notes/tergite-frontend.html#section-1",
    "title": "Tergite Frontend",
    "section": "[2025.06.2]",
    "text": "[2025.06.2]\nreleased on: 17th of June 2025\n\nFixed\n\nFix silent errors in e2e tests on Github during cleanup",
    "crumbs": [
      "Release Notes",
      "Tergite Frontend"
    ]
  },
  {
    "objectID": "release_notes/tergite-frontend.html#section-2",
    "href": "release_notes/tergite-frontend.html#section-2",
    "title": "Tergite Frontend",
    "section": "[2025.06.1]",
    "text": "[2025.06.1]\nreleased on: 16th of June 2025\n\nFixed\n\nFix failing e2e tests on Github during cleanup",
    "crumbs": [
      "Release Notes",
      "Tergite Frontend"
    ]
  },
  {
    "objectID": "release_notes/tergite-frontend.html#section-3",
    "href": "release_notes/tergite-frontend.html#section-3",
    "title": "Tergite Frontend",
    "section": "[2025.06.0]",
    "text": "[2025.06.0]\nreleased on: 16th of June 2025\n\nChanged\n\nBREAKING: Removed v2 from MSS URL in env and docker configs\nMSS\n\nBREAKING: Replaced requirements.txt with pyproject.toml\nBREAKING: Upgraded lowest version of python supported to python 3.12\nBREAKING: Upgraded Fastapi to 0.115.12\nBREAKING: Upgraded pydantic to the latest v2\nBREAKING: Removed the v1/auth, v1/backends and v1/jobs endpoints\nBREAKING: Changed creation of jobs to require a body with required fields ‘device’ and ‘calibration_date’\nBREAKING: Disabled Mongo-style payloads when updating jobs to ensure more security by reducing capabilities exposed to HTTP\nBREAKING: Changed job update payloads to only accept the fields that are in the Job schema\nBREAKING: Changed the POST /v2/calibrations endpoint to receive only a single calibration result at a time\nBREAKING: Changed the GET /v2/calibrations/ endpoint to return a paginated response of calibrations\nBREAKING: Made ‘calibration_date’ a required property on the job object\nBREAKING: Changed the GET /v2/me/jobs endpoint to /me/jobs/\nBREAKING: Changed the GET /me/jobs endpoint to return a paginated response of jobs\nBREAKING: Changed /v2/ endpoints to / endpoints\nBREAKING: Remove version property from project schema\nBREAKING: Return paginated response on the GET /devices/ endpoint\nBREAKING: Return paginated response on the GET /auth/providers/ endpoint\nBREAKING: Change the query param ‘domain’ on the GET /auth/providers/ endpoint to ‘email_domain’\n\nDashboard\n\nUpdated API client code to work with new endpoint structure in MSS\nUpdated cypress tests to work with new endpoint structure in MSS\nUpdated schema for devices in the fixtures\n\n\n\n\nAdded\n\nMSS\n\nAdded more extensive search capability on the GET /jobs/ endpoints so that one can search by more fields in the job\nAdded more extensive search capability on the GET /devices/ endpoints so that one can search by more fields in the devices\nAdded more extensive search capability on the GET /calibrations/ endpoints so that one can search by more fields in the calibrations\nAdded more extensive search capability on the GET /auth/providers/ endpoints so that one can search by more fields in the auth providers",
    "crumbs": [
      "Release Notes",
      "Tergite Frontend"
    ]
  },
  {
    "objectID": "release_notes/tergite-frontend.html#section-4",
    "href": "release_notes/tergite-frontend.html#section-4",
    "title": "Tergite Frontend",
    "section": "[2025.03.1]",
    "text": "[2025.03.1]\nrelease on: 18th of March 2025\n\nChanged\n\nChanged the backend configurations for the full e2e to run with the new backend configuration files",
    "crumbs": [
      "Release Notes",
      "Tergite Frontend"
    ]
  },
  {
    "objectID": "release_notes/tergite-frontend.html#section-5",
    "href": "release_notes/tergite-frontend.html#section-5",
    "title": "Tergite Frontend",
    "section": "[2025.03.0]",
    "text": "[2025.03.0]\nrelease on: 14th of March 2025\n\nAdded\n\nAdded Full end-to-end testing combining the backend, API and dashboard\nDashboard:\n\nAdded dark mode color change on dark mode toggle button click\nAdded a dark mode toggle button on the login page\nAdded a dark mode toggle button on the dashboard page\nChanged the coloring of the device map chart to use muted connection lines\nChanged the coloring of the device bar chart to use purple bar lines, and muted axes\n\n\n\n\nChanged\n\nMSS:\n\nRenamed quantum_jobs to jobs service\nRemoved the api routes, DTOs and service methods for the old WebGUI project.\n\nDashboard:\n\nDisabled the “live” switch when creating projects as admin\n\n\n\n\nFixed\n\nFix project list in nav bar to be only active projects",
    "crumbs": [
      "Release Notes",
      "Tergite Frontend"
    ]
  },
  {
    "objectID": "release_notes/tergite-frontend.html#section-6",
    "href": "release_notes/tergite-frontend.html#section-6",
    "title": "Tergite Frontend",
    "section": "[2024.12.2]",
    "text": "[2024.12.2]\nreleased on: 22nd of January 2025\n\nChanged\n\nRaised logging level to WARN in tergite-frontend when app settings are not production\nRemoved NETWORK_MODE environment variable in tergite-mss\n\n\n\nFixed\n\nFixed auth openid errors in tergite-mss when auth is disabled",
    "crumbs": [
      "Release Notes",
      "Tergite Frontend"
    ]
  },
  {
    "objectID": "release_notes/tergite-frontend.html#section-7",
    "href": "release_notes/tergite-frontend.html#section-7",
    "title": "Tergite Frontend",
    "section": "[2024.12.1]",
    "text": "[2024.12.1]\nreleased on: 20th of December 2024\n\nAdded\n\nDashboard:\n\nAdded run-nginx.sh script to help initialize/update the variables like cookie names API urls and the like in docker prebuilt containers.\n\n\n\n\nChanged\n\nUpdated the Github actions to push the ‘:latest’ tag along side the version tag to docker registry\nUpdated prebuilt-docker-compose.yml to pick relevant values from the environment as well as from mss-config.toml for the dashboard docker compose service\n\n\n\nFixed\n\nFixed the ‘parent-folder’ image not found error in the Github actions for the dashboard",
    "crumbs": [
      "Release Notes",
      "Tergite Frontend"
    ]
  },
  {
    "objectID": "release_notes/tergite-frontend.html#section-8",
    "href": "release_notes/tergite-frontend.html#section-8",
    "title": "Tergite Frontend",
    "section": "[2024.12.0]",
    "text": "[2024.12.0]\nreleased on: 11th of December 2024\n\nAdded\n\nAdded the admin pages for managing user access in the tergite-dashboard app.\n\nAdded the tokens list page for viewing, editing and deleting tokens of current user\nAdded the projects list page for viewing, requesting QPU time and deleting projects for current user\nAdded admin page for viewing, and approving user requests\nAdded admin page for viewing, editing, deleting and creating new projects\nAdded close button on the job detail drawer on the home page\n\nAdded user access endpoints to the v2 API version of the tergite-mss app.\n\nAdded GET /v2/me endpoint to get current user info\nAdded DELETE /v2/me/projects/{id} endpoint to delete project current user administers\nAdded GET /v2/admin/qpu-time-requests endpoint to get all user requests to increase QPU seconds on a project\nAdded POST /v2/admin/qpu-time-requests endpoint for project members to request for more QPU seconds\nAdded GET /v2/admin/user-requests endpoint for admins to retrieve a list of user requests\nAdded PUT /v2/admin/user-requests/{id} endpoint for admins to update (e.g. approve/reject) as user request\nAdded POST /v2/admin/projects/ endpoint for admins to create a new project\n\nCreates new empty user if non-existent emails are passed as user_emails or admin_email\n\nAdded GET /v2/admin/projects/ endpoint for admins to retireve a list of projects\nAdded GET /v2/admin/projects/{id} endpoint for admins to view a project of given id\nAdded PUT /v2/admin/projects/{id} endpoint for admins to update a project of given id\n\nCreates new empty user if non-existent emails are passed as user_emails or admin_email\n\nAdded DELETE /v2/admin/projects/{id} endpoint for admins to soft delete a project\nAdded the PUT /auth/me/app-tokens/{token_id} endpoint for extending app token’s lifetimes\nAdded the PUT /v2/me/tokens/{token_id} endpoint for extending app token’s lifetimes\nAdded user access endpoints to the v2 API version of the tergite-mss app.\n\nAdded the MSS_PORT environment variable to the .env.example to configure the port on which tergite-mss runs on in docker compose.\nAdded the NETWORK_MODE environment variable to the .env.example to configure the network_mode to use in docker compose file\n\n\n\nFixed\n\ntergite-mss\n\nFixed the not found error when deleting expired app tokens\nFixed httpx version to 0.27.2 as 0.28.0 removes many deprecations that we were still dependent on in FastAPI testClient\n\n\n\n\nChanged\n\ntergite-dashboard\n\nChanged devices page to show ‘no devices found’ when no devices are available.\nChanged to show sidebar placeholder on admin user requests page when no user request is selected",
    "crumbs": [
      "Release Notes",
      "Tergite Frontend"
    ]
  },
  {
    "objectID": "release_notes/tergite-frontend.html#section-9",
    "href": "release_notes/tergite-frontend.html#section-9",
    "title": "Tergite Frontend",
    "section": "[2024.09.1]",
    "text": "[2024.09.1]\nreleased on: 24th of September 2024\n\nAdded\n\nAdded example scopes in the mss_config.example.toml in tergite-mss\nAdded units ‘Hz’ and ‘s’ to calibration data schema\n\n\n\nFixed\n\nFixed CORS error when dashboard and MSS are on different domains or subdomains\nFixed ‘AttributeError: ’NoneType’ object has no attribute ‘resource_usage’’ on GET /v2/me/jobs\n\n\n\nChanged\n\nRemoved archives folder in tergite-mss\nRemoved dev folder in tergite-mss\nChanged all calibration v2 properties optional\nNormalized calibration data to have frequencies in GHz and durations in microseconds in the dashboard",
    "crumbs": [
      "Release Notes",
      "Tergite Frontend"
    ]
  },
  {
    "objectID": "release_notes/tergite-frontend.html#section-10",
    "href": "release_notes/tergite-frontend.html#section-10",
    "title": "Tergite Frontend",
    "section": "[2024.09.0]",
    "text": "[2024.09.0]\nreleased on: 2nd of September 2024\n\nAdded\n\nAdded the tergite-dashboard app. It included:\n\nDevice summary list on the dashboard home\nJobs list on the dashboard home\nJobs detail drawer on the dashboard home\nDevice list page\nDevice detail page\nError page\n\nAdded v2 endpoints on Tergite MSS including\n\n/v2/me/projects/ to GET, POST current cookie user’s projects\n/v2/me/projects/{project_id} to GET, PUT current cookie user’s single project\n/v2/me/tokens/ to GET, POST current cookie user’s application tokens\n/v2/me/tokens/{token_id} to GET, PUT current cookie user’s application token\n/v2/me/jobs/ to GET current cookie user’s jobs (with option of specifying project id)\n/v2/auth/providers to GET the available Oauth2 provider corresponding to a given email domain\n/v2/auth/{provider}/authorize to POST Oauth2 initialization request for given provider\n/v2/auth/{provider}/callback to handle GET redirects from 3rd party Oauth2 providers after successful login\n/v2/auth/logout to handle POST requests to logout the current user via cookies\n/v2/calibrations/ to GET, POST calibration data for all devices. POST is available for only system users.\n/v2/calibration/{device_name} to GET calibration data for the device of the given device_name\n/v2/devices to GET, PUT (upsert) all devices. PUT is available for only system users.\n/v2/devices/{name} to GET, PUT the device of a given name. PUT is available for only system users.\n\n\n\n\nChanged\n\nRemoved the tergite-landing-page app\nRemoved the tergite-webgui app",
    "crumbs": [
      "Release Notes",
      "Tergite Frontend"
    ]
  },
  {
    "objectID": "release_notes/tergite-frontend.html#section-11",
    "href": "release_notes/tergite-frontend.html#section-11",
    "title": "Tergite Frontend",
    "section": "[2024.04.1]",
    "text": "[2024.04.1]\nreleased on: 28th of May 2024\nInitial Public Release\n\nChanged\n\nUpdated the contribution guidelines and government model statements",
    "crumbs": [
      "Release Notes",
      "Tergite Frontend"
    ]
  },
  {
    "objectID": "release_notes/tergite-autocalibration.html#section",
    "href": "release_notes/tergite-autocalibration.html#section",
    "title": "Tergite Autocalibration",
    "section": "[2025.09.0] - 2025-09-16",
    "text": "[2025.09.0] - 2025-09-16\nreleased on 16th of September 2025\n\nAdded\n\nDatasets from IQT Nordics\n\n\n\nChanged\n\nData browser uses plotly instead of PyQT and integrates better with CLI\nRun single calibration node in re-analysis\nMigrated documentation from Quarto to MkDocs Material.\n\n\n\nFixed\n\nSimplify GitLab pipeline",
    "crumbs": [
      "Release Notes",
      "Tergite Autocalibration"
    ]
  },
  {
    "objectID": "release_notes/tergite-autocalibration.html#section-1",
    "href": "release_notes/tergite-autocalibration.html#section-1",
    "title": "Tergite Autocalibration",
    "section": "[2025.06.0]",
    "text": "[2025.06.0]\nreleased on 16th of June 2025\n\nAdded\n\nBetter labelling for the analysis output plots\n\n\n\nChanged\n\nUpgrade Python to version 3.12\nMigration from poetry to setuptools\nMake external samplespace multidimensional\n\n\n\nFixed\n\nReduced the packaged library size to below pypi’s limit by using a MANIFEST.in file",
    "crumbs": [
      "Release Notes",
      "Tergite Autocalibration"
    ]
  },
  {
    "objectID": "release_notes/tergite-autocalibration.html#section-2",
    "href": "release_notes/tergite-autocalibration.html#section-2",
    "title": "Tergite Autocalibration",
    "section": "[2025.03.0]",
    "text": "[2025.03.0]\nreleased on 16th of March 2025\n\nAdded\n\nAdvanced logging\nDebugging endpoint\nQuickstart endpoint to generate templates semi-automatically\n\n\n\nChanged\n\nRename all node class names to camel case\nRe-analysis is more user-friendly\nMake pipeline more modular\nImproved node documentation",
    "crumbs": [
      "Release Notes",
      "Tergite Autocalibration"
    ]
  },
  {
    "objectID": "release_notes/tergite-autocalibration.html#section-3",
    "href": "release_notes/tergite-autocalibration.html#section-3",
    "title": "Tergite Autocalibration",
    "section": "[2024.12.0]",
    "text": "[2024.12.0]\nreleased on 12th of December 2024\n\nAdded\n\nDataset browser\nScheduleNode and ExternalParameterSweepNode as subclasses of BaseNode\nDeviceManager class\n\n\n\nChanged\n\nMigrated cli from click to typer\nSwitch to quantify-scheduler version 0.21.2\nSwitch to qblox-instruments version 0.14.1 (qblox-firmware should be 9.0.1)\nUpgrade to Python version 3.10",
    "crumbs": [
      "Release Notes",
      "Tergite Autocalibration"
    ]
  },
  {
    "objectID": "release_notes/tergite-autocalibration.html#section-4",
    "href": "release_notes/tergite-autocalibration.html#section-4",
    "title": "Tergite Autocalibration",
    "section": "[2024.09.0]",
    "text": "[2024.09.0]\nreleased on 16th of September 2024\n\nAdded\n\nRedis storage manager\n\n\n\nChanged\n\nSwitch to quantify-scheduler version 0.20.0",
    "crumbs": [
      "Release Notes",
      "Tergite Autocalibration"
    ]
  },
  {
    "objectID": "release_notes/tergite-autocalibration.html#section-5",
    "href": "release_notes/tergite-autocalibration.html#section-5",
    "title": "Tergite Autocalibration",
    "section": "[2024.04.0]",
    "text": "[2024.04.0]\nreleased on 29th of May 2024\nInitial Public Release\n\nAdded\n\nAll research-related features regarding the calibration of a CZ gate\nUpdater to push calibration values as a backend to MSS/database\n\n\n\nChanged\n\nImproved command line interface\nRenamed from tergite-acl to tergite-autocalibration\nUpdated the contribution guidelines and government model statements",
    "crumbs": [
      "Release Notes",
      "Tergite Autocalibration"
    ]
  },
  {
    "objectID": "examples/03_equal_superposition.html",
    "href": "examples/03_equal_superposition.html",
    "title": "Equal Superposition",
    "section": "",
    "text": "This is a showcase of connecting to tergite via the tergite SDK, running a basic one-qubit circuit to produce the equal superposition state \\(|\\Psi\\rangle = |0\\rangle + |1\\rangle\\), and retrieving the measurement results.",
    "crumbs": [
      "Examples",
      "Equal Superposition"
    ]
  },
  {
    "objectID": "examples/03_equal_superposition.html#install-dependencies",
    "href": "examples/03_equal_superposition.html#install-dependencies",
    "title": "Equal Superposition",
    "section": "Install dependencies",
    "text": "Install dependencies\nThis example depends on:\n\nqiskit\ntergite\n\nInstall these dependencies\n\n%pip install qiskit\n%pip install tergite",
    "crumbs": [
      "Examples",
      "Equal Superposition"
    ]
  },
  {
    "objectID": "examples/03_equal_superposition.html#import-the-basic-dependencies",
    "href": "examples/03_equal_superposition.html#import-the-basic-dependencies",
    "title": "Equal Superposition",
    "section": "Import the basic dependencies",
    "text": "Import the basic dependencies\n\nimport time\nimport qiskit.circuit as circuit\nimport qiskit.compiler as compiler\nfrom tergite.qiskit.providers import Tergite\nfrom tergite.qiskit.providers.provider_account import ProviderAccount",
    "crumbs": [
      "Examples",
      "Equal Superposition"
    ]
  },
  {
    "objectID": "examples/03_equal_superposition.html#configure-session",
    "href": "examples/03_equal_superposition.html#configure-session",
    "title": "Equal Superposition",
    "section": "Configure Session",
    "text": "Configure Session\nBefore we get any further, we will take the time to define some of the parameters we will use for our tergite job.\n\n# the Tergite API URL e.g. \"https://api.tergite.example\"\nAPI_URL = \"https://api.qal9000.se\"\n# API token for connecting to tergite\nAPI_TOKEN = \"API-TOKEN\"\n# The name of the Quantum Computer to use from the available quantum computers\nBACKEND_NAME = \"SimulatorC\"\n# the name of this service. For your own bookkeeping.\nSERVICE_NAME = \"local\"\n# the timeout in seconds for how long to keep checking for results\nPOLL_TIMEOUT = 300",
    "crumbs": [
      "Examples",
      "Equal Superposition"
    ]
  },
  {
    "objectID": "examples/03_equal_superposition.html#get-the-tergite-backend",
    "href": "examples/03_equal_superposition.html#get-the-tergite-backend",
    "title": "Equal Superposition",
    "section": "Get the Tergite Backend",
    "text": "Get the Tergite Backend\nThe backend object can now be obtained. A detailed list of the backend properties — such as the available gate set, coupling map and number of qubits — is availablde by printing the backend object.\n\n# provider account creation can be skipped in case you already saved\n# your provider account to the `~/.qiskit/tergiterc` file.\n# See below how that is done.\naccount = ProviderAccount(service_name=SERVICE_NAME, url=API_URL, token=API_TOKEN)\n\nprovider = Tergite.use_provider_account(account)\n# to save this account to the `~/.qiskit/tergiterc` file, add the `save=True`\n# provider = Tergite.use_provider_account(account, save=True)\n\n# Get the tergite backend in case you skipped provider account creation\n# provider = Tergite.get_provider(service_name=SERVICE_NAME)\nbackend = provider.get_backend(BACKEND_NAME)\nbackend.set_options(shots=1024)\nprint(backend)",
    "crumbs": [
      "Examples",
      "Equal Superposition"
    ]
  },
  {
    "objectID": "examples/03_equal_superposition.html#create-the-qiskit-circuit",
    "href": "examples/03_equal_superposition.html#create-the-qiskit-circuit",
    "title": "Equal Superposition",
    "section": "Create the Qiskit Circuit",
    "text": "Create the Qiskit Circuit\nTo test our connection, we will implement a short test circuit. The circuit we will run produces the equal superposition state \\(|\\Psi\\rangle = |0\\rangle + |1\\rangle\\).\n\nqc = circuit.QuantumCircuit(1)\nqc.h(0)\n\nWe can visualize and verify our circuit with Qiskit’s built in draw() method. The output format of qc.draw() can be changed, see https://docs.quantum.ibm.com/build/circuit-visualization. Note the added measurement and corresponding classical bit register meas_0.\n\nqc.draw()\n\nTo measure the prepared Bell state we add explicit measurements to all qubits using qc.measure_all(). This will perform a meaurement in the so-called computational basis, \\(\\langle q|Z|q\\rangle\\), mapping the eigenvalues \\(\\{-1,1\\}\\) to the classical binary values \\(\\{0,1\\}\\). Drawing the final circuit shows the additional measurement operations and the classical bit register meas.\n\nqc.measure_all()\nqc.draw()",
    "crumbs": [
      "Examples",
      "Equal Superposition"
    ]
  },
  {
    "objectID": "examples/03_equal_superposition.html#compile-circuit",
    "href": "examples/03_equal_superposition.html#compile-circuit",
    "title": "Equal Superposition",
    "section": "Compile Circuit",
    "text": "Compile Circuit\nIn order to execute the circuit on physical hardware, the circuit needs to be compiled (or transpiled) to the target architecture. At the least, transpilation accounts for the QPU’s native gate set and the qubit connectivity on the QPU. Many transpilers also offer some level of optimization, reducing the circuit size.\n\ntc = compiler.transpile(qc, backend=backend)\ntc.draw()",
    "crumbs": [
      "Examples",
      "Equal Superposition"
    ]
  },
  {
    "objectID": "examples/03_equal_superposition.html#run-the-circuit",
    "href": "examples/03_equal_superposition.html#run-the-circuit",
    "title": "Equal Superposition",
    "section": "Run the Circuit",
    "text": "Run the Circuit\nOnce the cicruit has been compiled to the native gate set and connectivity, we use it to submit a job to the backend.\n\njob = backend.run(tc, meas_level=2, meas_return=\"single\")",
    "crumbs": [
      "Examples",
      "Equal Superposition"
    ]
  },
  {
    "objectID": "examples/03_equal_superposition.html#see-the-results",
    "href": "examples/03_equal_superposition.html#see-the-results",
    "title": "Equal Superposition",
    "section": "See the Results",
    "text": "See the Results\nWhen the job has been submitted, we will need to wait potential queue time and time required to execute the job.\n\nelapsed_time = 0\nresult = None\nwhile result is None:\n    if elapsed_time &gt; POLL_TIMEOUT:\n        raise TimeoutError(f\"result polling timeout {POLL_TIMEOUT} seconds exceeded\")\n\n    time.sleep(1)\n    elapsed_time += 1\n    result = job.result()\n\nresult.get_counts()",
    "crumbs": [
      "Examples",
      "Equal Superposition"
    ]
  },
  {
    "objectID": "examples/03_equal_superposition.html#acknowledgement",
    "href": "examples/03_equal_superposition.html#acknowledgement",
    "title": "Equal Superposition",
    "section": "Acknowledgement",
    "text": "Acknowledgement\nThis notebook was prepared by:\n\nMårten Skogh\nMartin Ahindura",
    "crumbs": [
      "Examples",
      "Equal Superposition"
    ]
  },
  {
    "objectID": "examples/01_hello_tergite.html",
    "href": "examples/01_hello_tergite.html",
    "title": "Hello Tergite",
    "section": "",
    "text": "This is a showcase of connecting to tergite via the tergite SDK and running a basic circuit.",
    "crumbs": [
      "Examples",
      "Hello Tergite"
    ]
  },
  {
    "objectID": "examples/01_hello_tergite.html#install-dependencies",
    "href": "examples/01_hello_tergite.html#install-dependencies",
    "title": "Hello Tergite",
    "section": "Install dependencies",
    "text": "Install dependencies\nThis example depends on:\n\nqiskit\ntergite\n\n\n%pip install qiskit\n%pip install tergite",
    "crumbs": [
      "Examples",
      "Hello Tergite"
    ]
  },
  {
    "objectID": "examples/01_hello_tergite.html#import-the-basic-dependencies",
    "href": "examples/01_hello_tergite.html#import-the-basic-dependencies",
    "title": "Hello Tergite",
    "section": "Import the basic dependencies",
    "text": "Import the basic dependencies\n\nimport time\nimport qiskit.circuit as circuit\nimport qiskit.compiler as compiler\nfrom tergite.qiskit.providers import Tergite\nfrom tergite.qiskit.providers.provider_account import ProviderAccount",
    "crumbs": [
      "Examples",
      "Hello Tergite"
    ]
  },
  {
    "objectID": "examples/01_hello_tergite.html#update-some-variables",
    "href": "examples/01_hello_tergite.html#update-some-variables",
    "title": "Hello Tergite",
    "section": "Update Some Variables",
    "text": "Update Some Variables\n\n# the Tergite API URL e.g. \"https://api.tergite.example\"\nAPI_URL = \"https://api.qal9000.se\"\n# API token for connecting to tergite\nAPI_TOKEN = \"&lt;your Tergite API key &gt;\"\n# The name of the Quantum Computer to use from the available quantum computers\nBACKEND_NAME = \"SimulatorC\"\n# the name of this service. For your own bookkeeping.\nSERVICE_NAME = \"local\"\n# the timeout in seconds for how long to keep checking for results\nPOLL_TIMEOUT = 100",
    "crumbs": [
      "Examples",
      "Hello Tergite"
    ]
  },
  {
    "objectID": "examples/01_hello_tergite.html#create-the-qiskit-circuit",
    "href": "examples/01_hello_tergite.html#create-the-qiskit-circuit",
    "title": "Hello Tergite",
    "section": "Create the Qiskit Circuit",
    "text": "Create the Qiskit Circuit\n\nqc = circuit.QuantumCircuit(1)\nqc.x(0)\nqc.h(0)\nqc.measure_all()\nqc.draw()",
    "crumbs": [
      "Examples",
      "Hello Tergite"
    ]
  },
  {
    "objectID": "examples/01_hello_tergite.html#get-the-tergite-backend",
    "href": "examples/01_hello_tergite.html#get-the-tergite-backend",
    "title": "Hello Tergite",
    "section": "Get the Tergite Backend",
    "text": "Get the Tergite Backend\n\n# provider account creation can be skipped in case you already saved\n# your provider account to the `~/.qiskit/tergiterc` file.\n# See below how that is done.\naccount = ProviderAccount(service_name=SERVICE_NAME, url=API_URL, token=API_TOKEN)\n\nprovider = Tergite.use_provider_account(account)\n# to save this account to the `~/.qiskit/tergiterc` file, add the `save=True`\n# provider = Tergite.use_provider_account(account, save=True)\n\n# Get the tergite backend in case you skipped provider account creation\n# provider = Tergite.get_provider(service_name=SERVICE_NAME)\nbackend = provider.get_backend(BACKEND_NAME)\nbackend.set_options(shots=1024)\nbackend",
    "crumbs": [
      "Examples",
      "Hello Tergite"
    ]
  },
  {
    "objectID": "examples/01_hello_tergite.html#compile-circuit",
    "href": "examples/01_hello_tergite.html#compile-circuit",
    "title": "Hello Tergite",
    "section": "Compile Circuit",
    "text": "Compile Circuit\n\ntc = compiler.transpile(qc, backend=backend)\ntc.draw()",
    "crumbs": [
      "Examples",
      "Hello Tergite"
    ]
  },
  {
    "objectID": "examples/01_hello_tergite.html#run-the-circuit",
    "href": "examples/01_hello_tergite.html#run-the-circuit",
    "title": "Hello Tergite",
    "section": "Run the Circuit",
    "text": "Run the Circuit\n\njob = backend.run(tc, meas_level=2, meas_return=\"single\")\n\nTergite: Job has been successfully submitted",
    "crumbs": [
      "Examples",
      "Hello Tergite"
    ]
  },
  {
    "objectID": "examples/01_hello_tergite.html#see-the-results",
    "href": "examples/01_hello_tergite.html#see-the-results",
    "title": "Hello Tergite",
    "section": "See the Results",
    "text": "See the Results\n\nelapsed_time = 0\nresult = None\nwhile result is None:\n    if elapsed_time &gt; POLL_TIMEOUT:\n        raise TimeoutError(f\"result polling timeout {POLL_TIMEOUT} seconds exceeded\")\n\n    time.sleep(1)\n    elapsed_time += 1\n    result = job.result()\n\nresult.get_counts()",
    "crumbs": [
      "Examples",
      "Hello Tergite"
    ]
  },
  {
    "objectID": "examples/01_hello_tergite.html#acknowledgement",
    "href": "examples/01_hello_tergite.html#acknowledgement",
    "title": "Hello Tergite",
    "section": "Acknowledgement",
    "text": "Acknowledgement\nThis notebook was prepared by:\n\nStefan Hill\nMartin Ahindura",
    "crumbs": [
      "Examples",
      "Hello Tergite"
    ]
  },
  {
    "objectID": "developer/system_design/5_sdk.html",
    "href": "developer/system_design/5_sdk.html",
    "title": "SDK",
    "section": "",
    "text": "TODO\n\n\n\n Back to top"
  },
  {
    "objectID": "developer/system_design/3_public_api.html",
    "href": "developer/system_design/3_public_api.html",
    "title": "Public API",
    "section": "",
    "text": "Tergite public API flows\n\n\n\n\nThe Tergite SDK sends a request to the Tergite public API to initialize the experiment for a given backend.\nThe public API creates an empty job with a unique ID and saves it in ites database. It responds to the SDK with the job ID and the URL of the backend.\n\n\n\nThe Tergite SDK sends the OpenPulse representation of the experiment(s) to the Tergite backend. The Tergite backend runs the experiments and sends the results to the Tergite public API app.\nThe public API app updates the job in the database with the status of the run and the results, if any, of the experiment(s).\n\n\n\nThe Tergite SDK then requests the Tergite public API for the results. The public API then reads the results from its database and responds to the SDK with them."
  },
  {
    "objectID": "developer/system_design/3_public_api.html#flows",
    "href": "developer/system_design/3_public_api.html#flows",
    "title": "Public API",
    "section": "",
    "text": "Tergite public API flows\n\n\n\n\nThe Tergite SDK sends a request to the Tergite public API to initialize the experiment for a given backend.\nThe public API creates an empty job with a unique ID and saves it in ites database. It responds to the SDK with the job ID and the URL of the backend.\n\n\n\nThe Tergite SDK sends the OpenPulse representation of the experiment(s) to the Tergite backend. The Tergite backend runs the experiments and sends the results to the Tergite public API app.\nThe public API app updates the job in the database with the status of the run and the results, if any, of the experiment(s).\n\n\n\nThe Tergite SDK then requests the Tergite public API for the results. The public API then reads the results from its database and responds to the SDK with them."
  },
  {
    "objectID": "developer/system_design/3_public_api.html#special-techniques",
    "href": "developer/system_design/3_public_api.html#special-techniques",
    "title": "Public API",
    "section": "Special Techniques",
    "text": "Special Techniques\n\nAuthentication\nTODO\n\n\nAccounting\nTODO"
  },
  {
    "objectID": "developer/system_design/1_overview.html",
    "href": "developer/system_design/1_overview.html",
    "title": "Overview",
    "section": "",
    "text": "The Tergite stack is composed of three components\n\nbackend\n\nThe operating system of the quantum computer. It communicates with the world via a RESTful API.\n\nfrontend\n\nThe user facing part of the system comprising:\n\n\ndashboard\n\nThe web-based user interface to take a peek into the quantum computer’s properties\n\n\n\nPublic API\n\nThe RESTful API to take a peek into the quantum computer’s properties\n\n\n\n\nSDK\n\nThe Python software development kit (SDK) quantum computer researchers can use in their scripts to interact with the Tergite stack.\n\n\n\n\n\nThe Tergite stack has a number of accessory softwares that are currently not initmately woven into the stack but mught be in future\n\nautomatic calibration\n\nThe Python commandline application (CLI) that is used to automatically tune the quantum computer."
  },
  {
    "objectID": "developer/system_design/1_overview.html#structure",
    "href": "developer/system_design/1_overview.html#structure",
    "title": "Overview",
    "section": "",
    "text": "The Tergite stack is composed of three components\n\nbackend\n\nThe operating system of the quantum computer. It communicates with the world via a RESTful API.\n\nfrontend\n\nThe user facing part of the system comprising:\n\n\ndashboard\n\nThe web-based user interface to take a peek into the quantum computer’s properties\n\n\n\nPublic API\n\nThe RESTful API to take a peek into the quantum computer’s properties\n\n\n\n\nSDK\n\nThe Python software development kit (SDK) quantum computer researchers can use in their scripts to interact with the Tergite stack.\n\n\n\n\n\nThe Tergite stack has a number of accessory softwares that are currently not initmately woven into the stack but mught be in future\n\nautomatic calibration\n\nThe Python commandline application (CLI) that is used to automatically tune the quantum computer."
  },
  {
    "objectID": "developer/limitations/4_dashboard.html",
    "href": "developer/limitations/4_dashboard.html",
    "title": "Dashboard",
    "section": "",
    "text": "TODO\n\n\n\n Back to top"
  },
  {
    "objectID": "developer/limitations/2_backend.html",
    "href": "developer/limitations/2_backend.html",
    "title": "Backend",
    "section": "",
    "text": "TODO\n\n\n\n Back to top"
  },
  {
    "objectID": "contributing/guidelines.html",
    "href": "contributing/guidelines.html",
    "title": "Contributing to tergite",
    "section": "",
    "text": "This project is currently not accepting pull requests from the general public yet.\nIt is currently being developed by the core developers only.\nWe love your input! We want to make contributing to this project as easy and transparent as possible, whether it’s:\n\nReporting a bug\nDiscussing the current state of the code\nSubmitting a fix\nProposing new features\nBecoming a maintainer\n\n\n\nChalmers Next Labs AB (CNL) manages and maintains this project on behalf of all contributors.\n\n\n\nTergite is developed on a separate version control system and mirrored on GitHub. If you are reading this on GitHub, then you are looking at a mirror.\n\n\n\nSince the GitHub repositories are only mirrors, no GitHub pull requests or GitHub issue/bug reports are looked at. Please get in touch via email quantum-nextlabs@chalmers.se instead.\nTake note that the maintainers may not answer every email.\n\n\n\nPull requests are the best way to propose changes to the codebase (we use GitHub Flow). We actively welcome your pull requests:\n\nClone the repo and create your branch from main.\nIf you’ve added code that should be tested, add tests.\nIf you’ve changed APIs, update the documentation.\nEnsure the test suite passes.\nMake sure your code lints.\nIssue that pull request!\n\n\n\n\nIn short, when you submit code changes, your submissions are understood to be under the same Apache 2.0 License that covers the project. Feel free to contact the maintainers if that’s a concern.\n\n\n\nThis is an example. Here’s another example from Craig Hockenberry.\nGreat Bug Reports tend to have:\n\nA quick summary and/or background\nSteps to reproduce\n\nBe specific!\nGive sample code if you can.\n\nWhat you expected would happen\nWhat actually happens\nNotes (possibly including why you think this might be happening, or stuff you tried that didn’t work)\n\nPeople love thorough bug reports. I’m not even kidding.\n\n\n\nBy contributing, you agree that your contributions will be licensed under its Apache 2.0 License.\n\n\n\nBefore you can submit any code, all contributors must sign a contributor license agreement (CLA). By signing a CLA, you’re attesting that you are the author of the contribution, and that you’re freely contributing it under the terms of the Apache-2.0 license.\n“The individual CLA document is available for review as a PDF.\nPlease note that if your contribution is part of your employment or your contribution is the property of your employer, you will also most likely need to sign a corporate CLA.\nAll signed CLAs are emails to us at quantum-nextlabs@chalmers.se.”\n\n\n\nThis document was adapted from a gist by Brian A. Danielak",
    "crumbs": [
      "Contributing",
      "Contributing to tergite"
    ]
  },
  {
    "objectID": "contributing/guidelines.html#government-model",
    "href": "contributing/guidelines.html#government-model",
    "title": "Contributing to tergite",
    "section": "",
    "text": "Chalmers Next Labs AB (CNL) manages and maintains this project on behalf of all contributors.",
    "crumbs": [
      "Contributing",
      "Contributing to tergite"
    ]
  },
  {
    "objectID": "contributing/guidelines.html#version-control",
    "href": "contributing/guidelines.html#version-control",
    "title": "Contributing to tergite",
    "section": "",
    "text": "Tergite is developed on a separate version control system and mirrored on GitHub. If you are reading this on GitHub, then you are looking at a mirror.",
    "crumbs": [
      "Contributing",
      "Contributing to tergite"
    ]
  },
  {
    "objectID": "contributing/guidelines.html#contacting-the-tergite-developers",
    "href": "contributing/guidelines.html#contacting-the-tergite-developers",
    "title": "Contributing to tergite",
    "section": "",
    "text": "Since the GitHub repositories are only mirrors, no GitHub pull requests or GitHub issue/bug reports are looked at. Please get in touch via email quantum-nextlabs@chalmers.se instead.\nTake note that the maintainers may not answer every email.",
    "crumbs": [
      "Contributing",
      "Contributing to tergite"
    ]
  },
  {
    "objectID": "contributing/guidelines.html#but-we-use-github-flow-so-all-code-changes-happen-through-pull-requests",
    "href": "contributing/guidelines.html#but-we-use-github-flow-so-all-code-changes-happen-through-pull-requests",
    "title": "Contributing to tergite",
    "section": "",
    "text": "Pull requests are the best way to propose changes to the codebase (we use GitHub Flow). We actively welcome your pull requests:\n\nClone the repo and create your branch from main.\nIf you’ve added code that should be tested, add tests.\nIf you’ve changed APIs, update the documentation.\nEnsure the test suite passes.\nMake sure your code lints.\nIssue that pull request!",
    "crumbs": [
      "Contributing",
      "Contributing to tergite"
    ]
  },
  {
    "objectID": "contributing/guidelines.html#any-contributions-you-make-will-be-under-the-apache-2.0-software-licenses",
    "href": "contributing/guidelines.html#any-contributions-you-make-will-be-under-the-apache-2.0-software-licenses",
    "title": "Contributing to tergite",
    "section": "",
    "text": "In short, when you submit code changes, your submissions are understood to be under the same Apache 2.0 License that covers the project. Feel free to contact the maintainers if that’s a concern.",
    "crumbs": [
      "Contributing",
      "Contributing to tergite"
    ]
  },
  {
    "objectID": "contributing/guidelines.html#write-bug-reports-with-detail-background-and-sample-code",
    "href": "contributing/guidelines.html#write-bug-reports-with-detail-background-and-sample-code",
    "title": "Contributing to tergite",
    "section": "",
    "text": "This is an example. Here’s another example from Craig Hockenberry.\nGreat Bug Reports tend to have:\n\nA quick summary and/or background\nSteps to reproduce\n\nBe specific!\nGive sample code if you can.\n\nWhat you expected would happen\nWhat actually happens\nNotes (possibly including why you think this might be happening, or stuff you tried that didn’t work)\n\nPeople love thorough bug reports. I’m not even kidding.",
    "crumbs": [
      "Contributing",
      "Contributing to tergite"
    ]
  },
  {
    "objectID": "contributing/guidelines.html#license",
    "href": "contributing/guidelines.html#license",
    "title": "Contributing to tergite",
    "section": "",
    "text": "By contributing, you agree that your contributions will be licensed under its Apache 2.0 License.",
    "crumbs": [
      "Contributing",
      "Contributing to tergite"
    ]
  },
  {
    "objectID": "contributing/guidelines.html#contributor-licensing-agreement",
    "href": "contributing/guidelines.html#contributor-licensing-agreement",
    "title": "Contributing to tergite",
    "section": "",
    "text": "Before you can submit any code, all contributors must sign a contributor license agreement (CLA). By signing a CLA, you’re attesting that you are the author of the contribution, and that you’re freely contributing it under the terms of the Apache-2.0 license.\n“The individual CLA document is available for review as a PDF.\nPlease note that if your contribution is part of your employment or your contribution is the property of your employer, you will also most likely need to sign a corporate CLA.\nAll signed CLAs are emails to us at quantum-nextlabs@chalmers.se.”",
    "crumbs": [
      "Contributing",
      "Contributing to tergite"
    ]
  },
  {
    "objectID": "contributing/guidelines.html#references",
    "href": "contributing/guidelines.html#references",
    "title": "Contributing to tergite",
    "section": "",
    "text": "This document was adapted from a gist by Brian A. Danielak",
    "crumbs": [
      "Contributing",
      "Contributing to tergite"
    ]
  },
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "Tergite",
    "section": "",
    "text": "Apache License\n                       Version 2.0, January 2004\n                    http://www.apache.org/licenses/\nTERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\nDefinitions.\n“License” shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document.\n“Licensor” shall mean the copyright owner or entity authorized by the copyright owner that is granting the License.\n“Legal Entity” shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, “control” means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity.\n“You” (or “Your”) shall mean an individual or Legal Entity exercising permissions granted by this License.\n“Source” form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files.\n“Object” form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types.\n“Work” shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below).\n“Derivative Works” shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof.\n“Contribution” shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, “submitted” means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as “Not a Contribution.”\n“Contributor” shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work.\nGrant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form.\nGrant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed.\nRedistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions:\n\nYou must give any other recipients of the Work or Derivative Works a copy of this License; and\nYou must cause any modified files to carry prominent notices stating that You changed the files; and\nYou must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and\nIf the Work includes a “NOTICE” text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License.\n\nYou may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License.\nSubmission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions.\nTrademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file.\nDisclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an “AS IS” BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License.\nLimitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages.\nAccepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability.\n\nEND OF TERMS AND CONDITIONS\nAPPENDIX: How to apply the Apache License to your work.\n  To apply the Apache License to your work, attach the following\n  boilerplate notice, with the fields enclosed by brackets \"[]\"\n  replaced with your own identifying information. (Don't include\n  the brackets!)  The text should be enclosed in the appropriate\n  comment syntax for the file format. We also recommend that a\n  file or class name and description of purpose be included on the\n  same \"printed page\" as the copyright notice for easier\n  identification within third-party archives.\nCopyright [yyyy] [name of copyright owner]\nLicensed under the Apache License, Version 2.0 (the “License”); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n   http://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on an “AS IS” BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n\n\n\n Back to top"
  },
  {
    "objectID": "developer/limitations/1_overview.html",
    "href": "developer/limitations/1_overview.html",
    "title": "Overview",
    "section": "",
    "text": "TODO\n\n\n\n Back to top"
  },
  {
    "objectID": "developer/limitations/3_public_api.html",
    "href": "developer/limitations/3_public_api.html",
    "title": "Public API",
    "section": "",
    "text": "TODO\n\n\n\n Back to top"
  },
  {
    "objectID": "developer/limitations/5_sdk.html",
    "href": "developer/limitations/5_sdk.html",
    "title": "SDK",
    "section": "",
    "text": "TODO\n\n\n\n Back to top"
  },
  {
    "objectID": "developer/system_design/2_backend.html",
    "href": "developer/system_design/2_backend.html",
    "title": "Backend",
    "section": "",
    "text": "Tergite backend flows\n\n\n\n\nThe Tergite SDK is used by the experimantalist to specify the quantum circuits or the pulses to run on the quantum computer, sending these instructions in OpenPulse format to the Tergite backend.\n\n\n\nThe Tergite backend receives the experiments as an HDF5 file sent to its RESTful API. It passes the file over to the quantum jobs queue for processing and execution.\n\n\n\n\n\nIn order to keep track of the experiment(s) while in the queue, it is registered in the database of Tergite backend. It is then passed onto the next worker: the preprocessing worker.\n\n\n\nCurrently, this worker does nothing of significance. The experiment(s) is then passed onto the execution worker.\n\n\n\nThe experiment(s) are then converted from OpenPulse into the format that the control instruments of the quantum computer understand and then forwarded to the control instruments to be executed.\nThe results are then passed on to the postprocessing worker.\n\n\n\nIn case the final results were expected to be discriminated, this worker discriminates them. Discriminating is the transformation of raw measurement values into ones and zeroes (and twos)\nThe worker saves the results in the database and then sends them to the public API app so that the experimentalist can query it from there."
  },
  {
    "objectID": "developer/system_design/2_backend.html#flows",
    "href": "developer/system_design/2_backend.html#flows",
    "title": "Backend",
    "section": "",
    "text": "Tergite backend flows\n\n\n\n\nThe Tergite SDK is used by the experimantalist to specify the quantum circuits or the pulses to run on the quantum computer, sending these instructions in OpenPulse format to the Tergite backend.\n\n\n\nThe Tergite backend receives the experiments as an HDF5 file sent to its RESTful API. It passes the file over to the quantum jobs queue for processing and execution.\n\n\n\n\n\nIn order to keep track of the experiment(s) while in the queue, it is registered in the database of Tergite backend. It is then passed onto the next worker: the preprocessing worker.\n\n\n\nCurrently, this worker does nothing of significance. The experiment(s) is then passed onto the execution worker.\n\n\n\nThe experiment(s) are then converted from OpenPulse into the format that the control instruments of the quantum computer understand and then forwarded to the control instruments to be executed.\nThe results are then passed on to the postprocessing worker.\n\n\n\nIn case the final results were expected to be discriminated, this worker discriminates them. Discriminating is the transformation of raw measurement values into ones and zeroes (and twos)\nThe worker saves the results in the database and then sends them to the public API app so that the experimentalist can query it from there."
  },
  {
    "objectID": "developer/system_design/2_backend.html#special-techniques",
    "href": "developer/system_design/2_backend.html#special-techniques",
    "title": "Backend",
    "section": "Special Techniques",
    "text": "Special Techniques\n\nTranspiling OpenPulse\nThe real control devices in current use are produced by Qblox. However, the simulators (COMING SOON) are developed on top of the qiskit dynamics.\nThere is therefore two types of native instrument targets to transpile OpenPulse to.\n\nReal Device (Qblox)\nQblox instruments can be controlled using the quantify python packages produced by Qblox itself. Transpilation is therefore from OpenPulse to quantify’s Schedule.\nTODO\n\nAdd the transpilation process as a diagram and as text\n\n\n\nSimulator (qiskit dynamics)\nTODO\n\nAdd introduction about what OpenPulse is transpiled to\nAdd the transpilation process as a diagram and as text"
  },
  {
    "objectID": "developer/system_design/4_dashboard.html",
    "href": "developer/system_design/4_dashboard.html",
    "title": "Dashboard",
    "section": "",
    "text": "TODO\n\n\n\n Back to top"
  },
  {
    "objectID": "developer/system_design/6_automatic_calibration.html",
    "href": "developer/system_design/6_automatic_calibration.html",
    "title": "Automatic calibration",
    "section": "",
    "text": "TODO\n\n\n\n Back to top"
  },
  {
    "objectID": "examples/02_bell_state.html",
    "href": "examples/02_bell_state.html",
    "title": "Bell State",
    "section": "",
    "text": "This is a showcase of connecting to tergite via the tergite SDK, running a basic two-qubit circuit to generate the bell state \\(|\\Psi\\rangle = |00\\rangle + |11\\rangle\\), and retrieving the measurement results.",
    "crumbs": [
      "Examples",
      "Bell State"
    ]
  },
  {
    "objectID": "examples/02_bell_state.html#install-dependencies",
    "href": "examples/02_bell_state.html#install-dependencies",
    "title": "Bell State",
    "section": "Install dependencies",
    "text": "Install dependencies\nThis example depends on:\n\nqiskit\ntergite\n\nInstall these dependencies\n\n%pip install qiskit\n%pip install tergite",
    "crumbs": [
      "Examples",
      "Bell State"
    ]
  },
  {
    "objectID": "examples/02_bell_state.html#import-the-basic-dependencies",
    "href": "examples/02_bell_state.html#import-the-basic-dependencies",
    "title": "Bell State",
    "section": "Import the basic dependencies",
    "text": "Import the basic dependencies\n\nimport time\nimport qiskit.circuit as circuit\nimport qiskit.compiler as compiler\nfrom tergite.qiskit.providers import Tergite\nfrom tergite.qiskit.providers.provider_account import ProviderAccount",
    "crumbs": [
      "Examples",
      "Bell State"
    ]
  },
  {
    "objectID": "examples/02_bell_state.html#configure-session",
    "href": "examples/02_bell_state.html#configure-session",
    "title": "Bell State",
    "section": "Configure Session",
    "text": "Configure Session\nBefore we get any further, we will take the time to define some of the parameters we will use for our tergite job.\n\n# the Tergite API URL e.g. \"https://api.tergite.example\"\nAPI_URL = \"https://api.qal9000.se\"\n# API token for connecting to tergite\nAPI_TOKEN = \"API-TOKEN\"\n# The name of the Quantum Computer to use from the available quantum computers\nBACKEND_NAME = \"SimulatorC\"\n# the name of this service. For your own bookkeeping.\nSERVICE_NAME = \"local\"\n# the timeout in seconds for how long to keep checking for results\nPOLL_TIMEOUT = 300",
    "crumbs": [
      "Examples",
      "Bell State"
    ]
  },
  {
    "objectID": "examples/02_bell_state.html#get-the-tergite-backend",
    "href": "examples/02_bell_state.html#get-the-tergite-backend",
    "title": "Bell State",
    "section": "Get the Tergite Backend",
    "text": "Get the Tergite Backend\nThe backend object can now be obtained. A detailed list of the backend properties — such as the available gate set, coupling map and number of qubits — is availablde by printing the backend object.\n\n# provider account creation can be skipped in case you already saved\n# your provider account to the `~/.qiskit/tergiterc` file.\n# See below how that is done.\naccount = ProviderAccount(service_name=SERVICE_NAME, url=API_URL, token=API_TOKEN)\n\nprovider = Tergite.use_provider_account(account)\n# to save this account to the `~/.qiskit/tergiterc` file, add the `save=True`\n# provider = Tergite.use_provider_account(account, save=True)\n\n# Get the tergite backend in case you skipped provider account creation\n# provider = Tergite.get_provider(service_name=SERVICE_NAME)\nbackend = provider.get_backend(BACKEND_NAME)\nbackend.set_options(shots=1024)\nprint(backend)",
    "crumbs": [
      "Examples",
      "Bell State"
    ]
  },
  {
    "objectID": "examples/02_bell_state.html#create-the-qiskit-circuit",
    "href": "examples/02_bell_state.html#create-the-qiskit-circuit",
    "title": "Bell State",
    "section": "Create the Qiskit Circuit",
    "text": "Create the Qiskit Circuit\nTo test our connection, we will implement a short test circuit. The circuit we will run produces the Bell state \\(|\\Psi\\rangle = |00\\rangle + |11\\rangle.\\)\n\nqc = circuit.QuantumCircuit(2)\nqc.h(0)\nqc.cx(0,1)\n\nWe can visualize and verify our circuit with Qiskit’s built in draw() method. The output format of qc.draw() can be changed, see https://docs.quantum.ibm.com/build/circuit-visualization. Note the two added measurements and corresponding classical bit registers meas_0 and meas_1.\n\nqc.draw()\n\nTo measure the prepared Bell state we add explicit measurements to all qubits using qc.measure_all(). This will perform a meaurement in the so-called computational basis, \\(\\langle q_n|Z|q_n\\rangle\\), mapping the eigenvalues \\(\\{-1,1\\}\\) to the classical binary values \\(\\{0,1\\}\\). Drawing the final circuit shows the additional measurement operations and the classical bit register meas_0 and meas_1.\n\nqc.measure_all()\nqc.draw()",
    "crumbs": [
      "Examples",
      "Bell State"
    ]
  },
  {
    "objectID": "examples/02_bell_state.html#compile-circuit",
    "href": "examples/02_bell_state.html#compile-circuit",
    "title": "Bell State",
    "section": "Compile Circuit",
    "text": "Compile Circuit\nIn order to execute the circuit on physical hardware, the circuit needs to be compiled (or transpiled) to the target architecture. At the least, transpilation accounts for the QPU’s native gate set and the qubit connectivity on the QPU. Many transpilers also offer some level of optimization, reducing the circuit size.\n\ntc = compiler.transpile(qc, backend=backend)\ntc.draw()",
    "crumbs": [
      "Examples",
      "Bell State"
    ]
  },
  {
    "objectID": "examples/02_bell_state.html#run-the-circuit",
    "href": "examples/02_bell_state.html#run-the-circuit",
    "title": "Bell State",
    "section": "Run the Circuit",
    "text": "Run the Circuit\nOnce the cicruit has been compiled to the native gate set and connectivity, we use it to submit a job to the backend.\n\njob = backend.run(tc, meas_level=2, meas_return=\"single\")",
    "crumbs": [
      "Examples",
      "Bell State"
    ]
  },
  {
    "objectID": "examples/02_bell_state.html#see-the-results",
    "href": "examples/02_bell_state.html#see-the-results",
    "title": "Bell State",
    "section": "See the Results",
    "text": "See the Results\nWhen the job has been submitted, we will need to wait potential queue time and time required to execute the job.\n\nelapsed_time = 0\nresult = None\nwhile result is None:\n    if elapsed_time &gt; POLL_TIMEOUT:\n        raise TimeoutError(f\"result polling timeout {POLL_TIMEOUT} seconds exceeded\")\n\n    time.sleep(1)\n    elapsed_time += 1\n    result = job.result()\n\nresult.get_counts()",
    "crumbs": [
      "Examples",
      "Bell State"
    ]
  },
  {
    "objectID": "examples/02_bell_state.html#acknowledgement",
    "href": "examples/02_bell_state.html#acknowledgement",
    "title": "Bell State",
    "section": "Acknowledgement",
    "text": "Acknowledgement\nThis notebook was prepared by:\n\nMårten Skogh\nMartin Ahindura",
    "crumbs": [
      "Examples",
      "Bell State"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tergite",
    "section": "",
    "text": "Quick Start\n  \n  \n    \n     GitHub\n  \n  \n    \n     Why Tergite?"
  },
  {
    "objectID": "index.html#structure",
    "href": "index.html#structure",
    "title": "Tergite",
    "section": "Structure",
    "text": "Structure\n\n\nComponents\nThe Tergite stack is composed of three components\n\ntergite-backend\n\nThe operating system of the quantum computer. It communicates with the world via a RESTful API.\n\ntergite-frontend\n\nThe server that gives remote access to any tergite-backend instance. It communicates with the world via a RESTful API.\n\nTergite SDK (or just tergite)\n\nThe Python software development kit (SDK) quantum computer researchers can use in their scripts to interact with the Tergite stack.\n\n\n\n\nAccessories\nThe Tergite stack has a number of accessory softwares that are currently not initmately woven into the stack but mught be in future\n\ntergite-autocalibration\n\nThe Python commandline application (CLI) that is used to automatically tune the quantum computer.\n\n\n\n\nLanguage of Communication\nWe use OpenPulse to communicate pass quantum computer instructions from the SDK to the backend part of the Tergite stack."
  },
  {
    "objectID": "index.html#development",
    "href": "index.html#development",
    "title": "Tergite",
    "section": "Development",
    "text": "Development\nThis project is developed by a core group of collaborators. Chalmers Next Labs AB (CNL) takes on the role of managing and maintaining this project.\n\nContributors\nWe are grateful for the wonderful contributions from:\n\nChalmers Next Labs AB - WACQT Quantum Technology Testbed\nChalmers University of Technology - Microtechnology and Nanoscience department\n\n\n\n\n\n\n\n \n\n\n \n\n\n\n\n\nSponsors\n\nKnut and Alice Wallenburg Foundation under the Wallenberg Center for Quantum Technology (WAQCT) project at Chalmers University of Technology\nNordic e-Infrastructure Collaboration (NeIC) and NordForsk under the NordIQuEst project\nEuropean Union’s Horizon Europe under the OpenSuperQ project\nEuropean Union’s Horizon Europe under the OpenSuperQPlus project"
  },
  {
    "objectID": "release_notes/tergite-backend.html#section-1",
    "href": "release_notes/tergite-backend.html#section-1",
    "title": "Tergite Backend",
    "section": "[2025.09.0]",
    "text": "[2025.09.0]\nreleased on: 2nd of October 2025\n\nAdded\n\nInstall a pre-commit hook to run black\nAdded a support for measurement level 1 for simulator for both single and avg modes\nAdded a support for measurement level 1 for real backend for both single and avg modes\nAdded authentication via JWT, with temporary JWT token generated by the backend instance and passed through MSS to the SDK\nAdded support for booked time slots, on top of the first-in-first-out (FIFO) queue\nAdded the following endpoints for the sake of handling bookings:\n\n/token (POST)\n/users (POST, GET)\n/users/{user_id} (POST, GET, DELETE)\n/me (DELETE, GET)\n/bookings (POST, GET)\n/bookings/{booking_id}/cancel (POST)\n\nBREAKING: Added authentication of requests from MSS by verifying that x-mss-signature is signed using MSS’s private key\nAdded canceling of jobs via MSS\nAdded and adapted SpiDAC class from autocalibration package for coupler biasing\nModify Quantify quantum executor by calling SpiDAC to bring up currents before running a schedule\nAdd a placeholder Wacqt-two-qubit-gate function for Qobj to QuantifySchedule mapper with a flux port reference\nAdd filtering bookings by minimum/maximum start timestamps\n\n\n\nChanged\n\nBREAKING: Changed authentication during job submission to use JWT token only\nBREAKING: Removed authentication of requests by referer’s IP address\nBREAKING: Removed authentication of job-submission requests that would check that the API key submitted was associated with the job submitted\n\nBREAKING: Removed the /auth endpoint\n\nBREAKING: Changed the return type of the /jobs POST endpoint to be the job in JSON as opposed to {\"message\": string}\nBREAKING: Added pagination to the /jobs GET endpoint, changing the return type to {\"data\": []object, limit: number, skip: number} as opposed to []object\nBREAKING: Changed the /jobs endpoint to return only jobs that belong to the user whose id is in the x-mss-user-is header.\nBREAKING: Changed the return type of the /jobs GET endpoint to be the job in JSON as opposed to {\"message\": object}\nBREAKING: Removed the /jobs/{job_id}/status endpoint.\nBREAKING: Removed the /jobs/{job_id}/result endpoint.\nBREAKING: Changed the return type of the /jobs DELETE endpoint to {\"status\": \"success\", \"detail\": string} as opposed to {\"message\": string}\nBREAKING: Changed the return type of the /jobs/{job_id}/cancel POST endpoint to {\"status\": \"success\", \"detail\": string} as opposed to nothing\nBREAKING: Changed the return type of the /logfiles/{logfile_id} GET endpoint when an error occurs to return an error status code with {\"detail\": string} JSON response.\nBREAKING: Changed the names of the workers/queues used to {PREFIX}_general, {PREFIX}_preprocessing, {PREFIX}_normal_execution, {PREFIX}_booked_execution, {PREFIX}_postprocessing\n\n\n\nFixed\n\nFix bcrypt 5.0.0 ValueError ‘password cannot be longer than 72 bytes, truncate manually if necessary (e.g. my_password[:72])’",
    "crumbs": [
      "Release Notes",
      "Tergite Backend"
    ]
  },
  {
    "objectID": "release_notes/tergite-backend.html#section-2",
    "href": "release_notes/tergite-backend.html#section-2",
    "title": "Tergite Backend",
    "section": "[2025.06.2]",
    "text": "[2025.06.2]\nreleased on: 17th of June 2025\n\nNo changes",
    "crumbs": [
      "Release Notes",
      "Tergite Backend"
    ]
  },
  {
    "objectID": "release_notes/tergite-backend.html#section-3",
    "href": "release_notes/tergite-backend.html#section-3",
    "title": "Tergite Backend",
    "section": "[2025.06.1]",
    "text": "[2025.06.1]\nreleased on: 17th of June 2025\n\nNo changes",
    "crumbs": [
      "Release Notes",
      "Tergite Backend"
    ]
  },
  {
    "objectID": "release_notes/tergite-backend.html#section-4",
    "href": "release_notes/tergite-backend.html#section-4",
    "title": "Tergite Backend",
    "section": "[2025.06.0]",
    "text": "[2025.06.0]\nreleased on: 16th of June 2025\n\nAdded\n\nAdd a simpler JSON document based store in redis\n\n\n\nChanged\n\nChanged source of lda parameters to backend’s redis store. Originally they were retrieved from MSS.\nAdded proper HTTP status codes for failed REST API requests\n\nInvalidJobIdInUploadedFileError results in a 400 HTTP response (originally was 200)\nItemNotFoundError results in a 404 HTTP response (originally was 200)\nJobAlreadyCancelled results in a 406 HTTP response (originally was 500)\n\nBREAKING: Change endpoint /v2/static-properties to /static-properties\nBREAKING: Change endpoint /v2/dynamic-properties to /dynamic-properties\nRename the properties library to device_parameters\nUpgraded to python 3.12\nRemoved the deprecated qiskit-ibmq-providers dependency, moving its code into our lib folder\nMoved from requirements.txt to pyproject.toml setup\nBumped up the version of fastapi to the latest (&gt;=0.115.12)\nAdded validation of job files when uploading jobs to /jobs",
    "crumbs": [
      "Release Notes",
      "Tergite Backend"
    ]
  },
  {
    "objectID": "release_notes/tergite-backend.html#section-5",
    "href": "release_notes/tergite-backend.html#section-5",
    "title": "Tergite Backend",
    "section": "[2025.03.2]",
    "text": "[2025.03.2]\nreleased on: 7th of April 2025\n\nChanged\n\nRemove stale fixtures\n\n\n\nFixed\n\nFixed JSONDecodeError when application is run in systemd",
    "crumbs": [
      "Release Notes",
      "Tergite Backend"
    ]
  },
  {
    "objectID": "release_notes/tergite-backend.html#section-6",
    "href": "release_notes/tergite-backend.html#section-6",
    "title": "Tergite Backend",
    "section": "[2025.03.1]",
    "text": "[2025.03.1]\nreleased on: 18th of March 2025\n\nChanged\n\nNo change",
    "crumbs": [
      "Release Notes",
      "Tergite Backend"
    ]
  },
  {
    "objectID": "release_notes/tergite-backend.html#section-7",
    "href": "release_notes/tergite-backend.html#section-7",
    "title": "Tergite Backend",
    "section": "[2025.03.0]",
    "text": "[2025.03.0]\nreleased on: 18th of March 2025\n\nChanged\n\nRemoved the storage-file internal lib\nLimited pyarrow to versions ‘18.0.0’ and below for macOS\nDecouple data from algorithms in storage file\nDecouple native job run from executor instance\nRemove debug prints of qobj when running jobs\nRename the run_experiment and run methods of the QuantumEexcutor class to _run_native, run\nClean up qobj-to-quantify compilation\nRemoved Program\nRemoved the dag property of the NativeExperiment class\nEnchanced the Channel class to track all instructions attached to it\nAdded the ‘to_operation’ method on the BaseInstruction of quantify, removing the long if-conditional QauntifyExperiment.schedule_operation method that was originally generating Operator’s\nReplaced the looping of the DAG with the looping of the instructions on each channel\nCreated the ChannelRegistry dict-like class to track the state of all channels attached to an experiment\nDeleted rot_left and ceil4 utility functions and other unused utility functions\nSplit FreqInstruction class to SetFreqInstruction and ShiftFreqInstruction\nSplit PhaseInstruction class to SetPhaseInstruction and ShiftPhaseInstruction\nAdded the QBLOX_TIMEGRID_INTERVAL constant\nMoved the Instruction class in the quantum_executor/base folder to quantum_executor/quantify folder, renaming it to BaseInstruction\nRemoved the channels property from the NativeExperiment class\nAdded the channel_registry property on the QauntifyExperiment class\nMoved the Channel definition from utils to the quantum_executor/quantify folder\nRemoved retworkx from the requirements.txt\nUpdated quantify-scheduler, quantify-core and qblox-instruments and pydantic versions\nBREAKING CHANGE: Split cluster configurations into quantify-config.json and quantify-metadata.json\nBREAKING CHANGE: Added a new configuration file calibration.seed.toml for seeding the database with calibration data\nEnhanced the QuantifySchedule conversion to use parametric schedules and new portclock convention",
    "crumbs": [
      "Release Notes",
      "Tergite Backend"
    ]
  },
  {
    "objectID": "release_notes/tergite-backend.html#section-8",
    "href": "release_notes/tergite-backend.html#section-8",
    "title": "Tergite Backend",
    "section": "[2024.12.1]",
    "text": "[2024.12.1]\nreleased on: 20th of December 2024\n\nAdded\n\nAdded Dockerfile.\nAdded instructions how to run with docker.\nAdded instructions how to run the qiskit_pulse_2q simulator in the configuration docs.\n\n\n\nChanged\n\nChanged start_bcc.sh script to use redis connection obtained from the environment.\nChanged start_bcc.sh script to update the exported environment variables after reading from the ENV_FILE.\nUpdated Github action to deploy built multiplatform image to docker hub as something like tergite/tergite-backend\nRemoved some redundant libraries in the requirements.txt file.\nRemoved some outdated docs.",
    "crumbs": [
      "Release Notes",
      "Tergite Backend"
    ]
  },
  {
    "objectID": "release_notes/tergite-backend.html#section-9",
    "href": "release_notes/tergite-backend.html#section-9",
    "title": "Tergite Backend",
    "section": "[2024.12.0]",
    "text": "[2024.12.0]\nreleased on: 11th of December 2024\n\nAdded\n\nAdded redis connection environment variables\nAdded GitLab CI configuration\nAdded storing of Qobj header data in the logfiles of the quantum jobs\nQiskit dynamics simulator backend with two-qubit CZ gate (“qiskit_pulse_2q”)\nAdded CouplerProps to Backend Configurations\nAdded the coupling_dict to the backend_config.toml\n\n\n\nFixed\n\nFixed httpx version to 0.27.2 as 0.28.0 removes many deprecations that we were still dependent on in FastAPI testClient\n\n\n\nChanged\n\nRemoved the coupling_map from the backend_config.toml as it is generated on-the-fly from the coupling_dict.",
    "crumbs": [
      "Release Notes",
      "Tergite Backend"
    ]
  },
  {
    "objectID": "release_notes/tergite-backend.html#section-10",
    "href": "release_notes/tergite-backend.html#section-10",
    "title": "Tergite Backend",
    "section": "[2024.09.1]",
    "text": "[2024.09.1]\nreleased on: 24th of September 2024\n\nFixed\n\nFixed ‘KeyError’ when no units are not passed in the backend_config file\nFixed “…bin/conda/activate: Not a directory” error when starting as systemd service\nFixed silent error where calibrations are not sent to MSS on executor initialization\nFixed “TypeError: Object of type datetime is not JSON serializable” when sending calibration data to MSS\nFixed ‘SyntaxWarning: ’is not’ with a literal’ when initializing backend",
    "crumbs": [
      "Release Notes",
      "Tergite Backend"
    ]
  },
  {
    "objectID": "release_notes/tergite-backend.html#section-11",
    "href": "release_notes/tergite-backend.html#section-11",
    "title": "Tergite Backend",
    "section": "[2024.09.0]",
    "text": "[2024.09.0]\nreleased on: 16th of September 2024\n\nAdded\n\nThe QuantumExecutor as abstract class to implement a backend\nEXECUTOR_TYPE keyword in the .env variables to select the backend\nQiskit dynamics simulator backend with one qubit (“qiskit_pulse_1q”)\nAdded the initialization of the redis store with configuration picked from the backend_config.toml file when the execution worker starts\nAdded an initial request to update the backend information in MSS when the execution worker starts\n\n\n\nChanged\n\nBREAKING CHANGE: EXECUTOR_DATA_DIRNAME definition in the .env variables instead of general.data_dir in executor-config.yml\nBREAKING CHANGE: Removed the whole general section in the executor-config.yml\nBREAKING CHANGE: Renamed executor-config.yml to quantify-config.yml\nRemoved the old config files that were used for setting up automatic calibration\nRemoved the script that loaded automatic calibration configurations at the start\nMoved the backend_config.toml file from /configs folder\nMoved the properties service to the libs folder\nMoved the storage, date_time, representation and logging utils to the properties lib\nRemoved the scripts folder",
    "crumbs": [
      "Release Notes",
      "Tergite Backend"
    ]
  },
  {
    "objectID": "release_notes/tergite-backend.html#section-12",
    "href": "release_notes/tergite-backend.html#section-12",
    "title": "Tergite Backend",
    "section": "[2024.04.0]",
    "text": "[2024.04.0]\nreleased on 28th of May 2024\nInitial Public Release\n\nAdded\n\nAdded storage_file lib (formerly tergite-quantify-connector-storagefile)\nAdded quantum_executor service (formerly tergite-quantify-connector)\nAdded the executor-config.yml and its python-based validators\n\n\n\nChanged\n\nChanged the way discriminators are loaded to load from the database\nBREAKING_CHANGE: Removed hard-coded discriminators\nBREAKING_CHANGE: Removed official support for Python 3.8; Official support is now &gt;=3.12\nBREAKING_CHANGE: Removed Labber support\nReplaced tergite-quantify-connector-storagefile package with an internal storage_file lib\nMoved unused files to archive folder\nBREAKING_CHANGE: Removed calibration and two state discrimination source code\nBREAKING_CHANGE: Replaced tergite-quantify-connector-storagefile package with an internal storage_file lib\nBREAKING_CHANGE: Merged tergite-quantify-connector into tergite-backend and renamed its service to quantum_executor\nBREAKING_CHANGE: Changed configuration of hardware to use executor-config.yml file with proper validations on loading\nBREAKING_CHANGE: Removed support for Pulsar, or any other instrument drivers other than Cluster\nThe old implementation wrongfully assumed that all these drivers have the same signature i.e. driver(name: str, identifier: str | None)\nyet SpiRack(name: str, address: str, baud_rate: int = 9600, timeout: float = 1, is_dummy: bool = False,),\nPulsar(name: str, identifier: Optional[str] = None, port: Optional[int] = None, debug: Optional[int] = None, dummy_type: Optional[PulsarType] = None,)\nCluster(name: str, identifier: Optional[str] = None, port: Optional[int] = None, debug: Optional[int] = None, dummy_type: Optional[PulsarType] = None) are all different.\n\nBREAKING_CHANGE: We got rid of quantify connector’s redundant reset() method.\nBREAKING_CHANGE: Changed backend name used when querying MSS for backend properties to be equal to settings.DEFAULT_PREFIX\n\n\n\nFixed\n\nFixed duplicate job uploads to respond with HTTP 409",
    "crumbs": [
      "Release Notes",
      "Tergite Backend"
    ]
  },
  {
    "objectID": "release_notes/tergite-sdk.html#section",
    "href": "release_notes/tergite-sdk.html#section",
    "title": "Tergite SDK",
    "section": "[2025.09.0]",
    "text": "[2025.09.0]\nreleased on: 2nd of October 2025\n\nAdded\n\nAdd e2e tests for meas_level 1 with clustering algorithm populaton and distance\n\n\n\nChanged\n\nExtend data type of the job results to allow IQpoints together with HexMap for meas_level 1\nClean up classical register of quantum circuits before scheduling them to remove idle classical registers\nChanged to use the access_token got on job submission to MSS, to submit/cancel jobs to BCC as well as download logfiles.",
    "crumbs": [
      "Release Notes",
      "Tergite SDK"
    ]
  },
  {
    "objectID": "release_notes/tergite-sdk.html#section-1",
    "href": "release_notes/tergite-sdk.html#section-1",
    "title": "Tergite SDK",
    "section": "[2025.06.1]",
    "text": "[2025.06.1]\nreleased on: 17th of June 2025\n\nChanged\n\nRemoved unwanted artefacts from the packaged library\n\n\n\nFixed\n\nMade cleanup in e2e fault tolerant",
    "crumbs": [
      "Release Notes",
      "Tergite SDK"
    ]
  },
  {
    "objectID": "release_notes/tergite-sdk.html#section-2",
    "href": "release_notes/tergite-sdk.html#section-2",
    "title": "Tergite SDK",
    "section": "[2025.06.0]",
    "text": "[2025.06.0]\nreleased on: 16th of June 2025\n\nAdded\n\nImplemented job cancellation\n\n\n\nChanged\n\nBREAKING: Changed the MSS endpoints /v2/devices to /devices\nBREAKING: Changed the MSS endpoints /v2/calibrations to /calibrations\nBREAKING: Changed the expected Device info schema to include ‘number_of_resonators’ and ‘number_of_couplers’\nAdded pydantic as an explicit dependency\nMoved completely from dataclasses to pydantic models\nSet the minimum permitted python to 3.12\nRemoved the tergite.qiskit package and moved everything to the tergite package\nRemoved the tergite.qiskit.providers package and moved everything to the parent package\nRaise QiskitBackendNotFoundError when malformed backend is returned in data instead of TypeError\nBREAKING: Remove extras from AccountInfo\nBREAKING: Change signature of Tergite.use_provider_account(account, save) to Tergite.use_provider_account(service_name, url, token, save)",
    "crumbs": [
      "Release Notes",
      "Tergite SDK"
    ]
  },
  {
    "objectID": "release_notes/tergite-sdk.html#section-3",
    "href": "release_notes/tergite-sdk.html#section-3",
    "title": "Tergite SDK",
    "section": "[2025.03.0]",
    "text": "[2025.03.0]\nreleased on: 14th of March 2025\n\nChanged\n\nRefactor: Changed a few error messages to include the original error\nRefactor: Changed the Job metadata to always be generated on the backend.\nIt was originally setting them both on the backend and the SDK.\nUpgraded to Poetry &gt; 2.1\n\n\n\nAdded\n\nAdded caching of calibration requests to reduce number of calls to remote API\nAdded caching on Job’s status(), result(), download_url to reduce number of calls to remote API\nAdded sending of calibration_date (str) query parameter during job registration.\nAdded end-to-end tests for qiskit_pulse_1q simulator backend\nAdded end-to-end tests for qiskit_pulse_2q simulator backend\nAdded Gitlab and Github CI configurations for end-to-end tests",
    "crumbs": [
      "Release Notes",
      "Tergite SDK"
    ]
  },
  {
    "objectID": "release_notes/tergite-sdk.html#section-4",
    "href": "release_notes/tergite-sdk.html#section-4",
    "title": "Tergite SDK",
    "section": "[2024.12.1]",
    "text": "[2024.12.1]\nreleased on: 18th of December 2024\n\nChanged\n\nUpdated tests to run tests against multiple backends\nUpdated Provider.job() to return a Job instance without an upload_url\n\n\n\nFixed\n\nFixed the error ‘Coupling (0, 0) not a coupling map’ for backends without couplers",
    "crumbs": [
      "Release Notes",
      "Tergite SDK"
    ]
  },
  {
    "objectID": "release_notes/tergite-sdk.html#section-5",
    "href": "release_notes/tergite-sdk.html#section-5",
    "title": "Tergite SDK",
    "section": "[2024.12.0]",
    "text": "[2024.12.0]\nreleased on: 11th of December 2024\n\nAdded\n\nAdded the job() method to provider to retrieve job by job ID\nAdded functionality to get control channel index given qubit tuple\nAdded two qubit CZ gate functionality\n\n\n\nChanged\n\nUpgraded Qiskit version to 1.0+, &lt;1.3\nUse MSS v2 endpoints for backend and calibration data",
    "crumbs": [
      "Release Notes",
      "Tergite SDK"
    ]
  },
  {
    "objectID": "release_notes/tergite-sdk.html#section-6",
    "href": "release_notes/tergite-sdk.html#section-6",
    "title": "Tergite SDK",
    "section": "[2024.09.0]",
    "text": "[2024.09.0]\nreleased on: 16th of September 2024\n\nNo change",
    "crumbs": [
      "Release Notes",
      "Tergite SDK"
    ]
  },
  {
    "objectID": "release_notes/tergite-sdk.html#section-7",
    "href": "release_notes/tergite-sdk.html#section-7",
    "title": "Tergite SDK",
    "section": "[2024.04.1]",
    "text": "[2024.04.1]\nreleased on: 29th of May 2024\nInitial Public Release\n\nChanged\n\nChanged README.rst to README.md\nChanged CONTRIBUTING.rst to CONTRIBUTING.md\nChanged CREDITS.rst to CREDITS.md\nUpdated the contribution guidelines and government model statements",
    "crumbs": [
      "Release Notes",
      "Tergite SDK"
    ]
  },
  {
    "objectID": "tutorials/02_adding_authentication.html",
    "href": "tutorials/02_adding_authentication.html",
    "title": "Adding authentication",
    "section": "",
    "text": "As a continuation of the quick start tutorial, we can enable authentication and authorization.\nLet’s do that now.",
    "crumbs": [
      "Tutorials",
      "Adding authentication"
    ]
  },
  {
    "objectID": "tutorials/02_adding_authentication.html#prerequisites",
    "href": "tutorials/02_adding_authentication.html#prerequisites",
    "title": "Adding authentication",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou must start from the quick start tutorial.",
    "crumbs": [
      "Tutorials",
      "Adding authentication"
    ]
  },
  {
    "objectID": "tutorials/02_adding_authentication.html#setup-the-backend",
    "href": "tutorials/02_adding_authentication.html#setup-the-backend",
    "title": "Adding authentication",
    "section": "Setup the Backend",
    "text": "Setup the Backend\n\nStart with the quick start tutorial’s ‘Setup the Backend’ section.\nStop the backend by pressing “Ctrl-C” on your keyboard.\nOpen the .env file with visual studio code (or any other text editor).\n\ncode .env\n\nUpdate .env file to have the following content\n\n# .env\nAPP_SETTINGS=development\nIS_AUTH_ENABLED=True \nMSS_APP_TOKEN=\"QPsDjT6LcDeSfW1HbYnE6xkhDRiN-2v4PSuUAo55tL0\"\n\nDEFAULT_PREFIX=loke\nSTORAGE_ROOT=/tmp\nLOGFILE_DOWNLOAD_POOL_DIRNAME=logfile_download_pool\nLOGFILE_UPLOAD_POOL_DIRNAME=logfile_upload_pool\nJOB_UPLOAD_POOL_DIRNAME=job_upload_pool\nJOB_PRE_PROC_POOL_DIRNAME=job_preproc_pool\nJOB_EXECUTION_POOL_DIRNAME=job_execution_pool\n\n# Main Service Server\nMSS_MACHINE_ROOT_URL=http://localhost:8002\nMSS_PORT=8002\n\n# Backend Control computer\nBCC_MACHINE_ROOT_URL=http://localhost:8000\nBCC_PORT=8000\n\nQUANTIFY_CONFIG_FILE=quantify-config.yml\n\nRun the start script\n\n./start_bcc.sh --device configs/device_default.toml\n\nOpen your browser at http://localhost:8000/docs to see the interactive API docs",
    "crumbs": [
      "Tutorials",
      "Adding authentication"
    ]
  },
  {
    "objectID": "tutorials/02_adding_authentication.html#setup-the-frontend",
    "href": "tutorials/02_adding_authentication.html#setup-the-frontend",
    "title": "Adding authentication",
    "section": "Setup the Frontend",
    "text": "Setup the Frontend\n\nStart with the quick start tutorial’s ‘Setup the Frontend’ section.\nStop the frontend.\n\ndocker compose -f fresh-docker-compose.yml down\n\nCreate a new GitHub Oauth2 application via the official instructions, with Homepage URL: “http://127.0.0.1:8002” and Authorization callback URL: “http://127.0.0.1:8002/”.\nCopy the client ID and the client secret of the GitHub Oauth2 application.\nOpen the mss-config.toml file in the tergite-frontend folder with visual studio code (or any other text editor).\n\ncode mss-config.toml\n\nUpdate the mss-config.toml with the following content\n\n# mss-config.toml\n\n# general configurations\n[general]\n# the port on which MSS is running\nmss_port = 8002\n# the port on which the websocket is running\nws_port = 6532\n# environment reflect which environment the app is to run in.\nenvironment = \"development\"\n# the host the uvicorn runs on.\n# During testing auth on 127.0.0.1, set this to \"127.0.0.1\". default: \"0.0.0.0\"\nmss_host = \"127.0.0.1\"\n\n[database]\n# configurations for the database\nname = \"testing\"\n# database URI\n# host.docker.internal resolves to the host's 127.0.0.1\n# see https://stackoverflow.com/questions/31324981/how-to-access-host-port-from-docker-container#answer-43541732\nurl = \"mongodb://host.docker.internal:27017\"\n\n[[backends]]\nname = \"loke\"\n# the URL where this backend is running\n# host.docker.internal resolves to the host's 127.0.0.1\n# see https://stackoverflow.com/questions/31324981/how-to-access-host-port-from-docker-container#answer-43541732\nurl = \"http://host.docker.internal:8000\"\n\n[auth]\n# turn auth OFF or ON, default=true\nis_enabled = true\njwt_secret = \"f236ffd1332dc9e38effd8fff582a69e56d72d7dae9a377f1fda5e0ebedeaa42\"\njwt_ttl = 3600\ncookie_domain = \"127.0.0.1\"\ncookie_name = \"tergiteauth\"\n\n[[auth.clients]]\nname = \"github\"\nclient_id = \"the-client-id-copied-from-github\"\nclient_secret = \"the-client-secret-copied-from-github\"\nredirect_url = \"http://127.0.0.1:8002/auth/github/callback\"\nclient_type = \"github\"\nemail_regex = \"^(john\\\\.doe|jane|your-gmail-username)@gmail\\\\.com$\"\nemail_domain = \"gmail.com\"\nroles = [\"admin\", \"user\"]\n\n# Puhuri synchronization\n# Puhuri is a resource management platform for HPC systems, that is also to be used for Quantum Computer's\n[puhuri]\n# turn puhuri synchronization OFF or ON, default=true\nis_enabled = false\n\n**Make sure you replace the your-gmail-username, the-client-id-copied-from-github, and the-client-secret-copied-from-github placeholders with the right values.\nDelete the old docker images of “tergite/tergite-mss”, “tergite/tergite-dashboard” from docker if they exist.\n\ndocker rmi tergite/tergite-mss:v0.0.1\ndocker rmi tergite/tergite-dashboard:v0.0.1\n\nTo Run the services, use the fresh-docker-compose.yml.\n\ndocker compose -f fresh-docker-compose.yml up -d\n\nOpen your browser at http://127.0.0.1:3000 to see the dashboard and attempt to login with github.\n\n\n\n\n\n\n\n\n\n\n\n\nAfter successful login, open the Mongo compass application and connect to the default local mongo database\n\n\n\n\n\n\n\n\n\n\n\n\nOpen the “auth_users” collection in the “testing” database in your mongo compass\nNote down the “_id” of the document that contains your details. Let us call it YOUR_USER_ID for now.\nDouble-click the “roles” field on the document containing your user details in the “auth_users” collection.\nClick the + button on the left side of the “roles” field and then click “Add item to roles” in the popup that appears.\n\n\n\n\n\n\n\n\n\n\n\n\nType in “system” in the new empty value that appears under roles\n\n\n\n\n\n\n\n\n\n\n\n\nClick on “ADD DATA” then “Insert document” in your “auth_app_tokens” collection in the “testing” database in your mongo compass\nCopy and paste the following document into the window that opens, replacing everything that was already there.\n\n{\n  \"_id\": {\n    \"$oid\": \"66bb23aaf19421387021040b\"\n  },\n  \"title\": \"test1-1723540394175\",\n  \"project_ext_id\": \"test1\",\n  \"lifespan_seconds\": 720000000,\n  \"token\": \"QPsDjT6LcDeSfW1HbYnE6xkhDRiN-2v4PSuUAo55tL0\",\n  \"user_id\": {\n    \"$oid\": \"YOUR_USER_ID\"\n  },\n  \"created_at\": {\n    \"$date\": \"CURRENT_TIMESTAMP\"\n  }\n}\n\n\"\"\n\n\n\n\n\n\nMake sure to replace the YOUR_USER_ID place holder with the actual string value from your database.\nMake sure to replace the CURRENT_TIMESTAMP place holder with ’  ’\n\nClick “Insert” to insert the document into the “auth_app_tokens” collection.\n\n\n\n\n\n\n\n\n\n\n\n\nClick on “ADD DATA” then “Insert document” in your “auth_projects” collection in the “testing” database in your mongo compass\nCopy and paste the following document into the window that opens, replacing everything that was already there.\n\n{\n  \"_id\": {\n    \"$oid\": \"66ba1b56f073eac3b195814c\"\n  },\n    \"version\": 2,\n    \"name\": \"Test 1\",\n    \"ext_id\": \"test1\",\n    \"admin_id\": \"YOUR_USER_ID\",\n    \"user_ids\": [ \"YOUR_USER_ID\"],\n    \"qpu_seconds\": 54000,\n    \"is_active\": true,\n    \"description\": \"This is a test project by the International Group of Other Testers\",\n    \"created_at\": \"2024-06-20T09:12:00.733Z\",\n    \"updated_at\": \"2024-06-20T09:12:00.733Z\"\n}\nMake sure to replace the YOUR_USER_ID place holder with the actual string value from your database.\n\nClick “Insert” to insert the document into the “auth_projects” collection.\n\n\n\n\n\n\n\n\n\n\n\n\nOpen the dashboard at http://127.0.0.1:3000\nSelect project “Test 1” in the top bar.\n\n\n\n\n\n\n\n\n\n\n\n\nGenerate a new api token and copy it to the clipboard. Let’s call it THE_API_TOKEN_YOU_CREATED for the sake of this tutorial.",
    "crumbs": [
      "Tutorials",
      "Adding authentication"
    ]
  },
  {
    "objectID": "tutorials/02_adding_authentication.html#run-an-experiment",
    "href": "tutorials/02_adding_authentication.html#run-an-experiment",
    "title": "Adding authentication",
    "section": "Run an Experiment",
    "text": "Run an Experiment\n\nStart with the quick start tutorial’s ‘Run an Experiment’ section.\nOpen the main.py file with visual studio code (or any other text editor).\n\ncode main.py\n\nUpdate the main.py file with the following content:\n\n# main.py\n\"\"\"A sample script doing a very simple quantum operation\"\"\"\nimport time\n\nimport qiskit.circuit as circuit\nimport qiskit.compiler as compiler\n\nfrom tergite import Job, Tergite\n\nif __name__ == \"__main__\":\n    # the Tergite API URL\n    API_URL = \"http://localhost:8002\"\n    # The name of the Quantum Computer to use from the available quantum computers\n    BACKEND_NAME = \"loke\"\n    # the application token for logging in\n    API_TOKEN = \"THE_API_TOKEN_YOU_CREATED\"\n    # the name of this service. For your own bookkeeping.\n    SERVICE_NAME = \"local\"\n    # the timeout in seconds for how long to keep checking for results\n    POLL_TIMEOUT = 100\n\n    # create the Qiskit circuit\n    qc = circuit.QuantumCircuit(1)\n    qc.x(0)\n    qc.h(0)\n    qc.measure_all()\n\n    # create a provider\n    # provider account creation can be skipped in case you already saved\n    # your provider account to the `~/.qiskit/tergiterc` file.\n    # See below how that is done.\n    provider = Tergite.use_provider_account(service_name=SERVICE_NAME, url=API_URL, token=API_TOKEN)\n    # to save this account to the `~/.qiskit/tergiterc` file, add the `save=True`\n    # provider = Tergite.use_provider_account(service_name=SERVICE_NAME, url=API_URL, token=API_TOKEN, save=True)\n\n    # Get the Tergite backend in case you skipped provider account creation\n    # provider = Tergite.get_provider(service_name=SERVICE_NAME)\n    backend = provider.get_backend(BACKEND_NAME)\n    backend.set_options(shots=1024)\n\n    # compile the circuit\n    tc = compiler.transpile(qc, backend=backend)\n\n    # run the circuit\n    job: Job = backend.run(tc, meas_level=2, meas_return=\"single\")\n    job.wait_for_final_state(timeout=POLL_TIMEOUT)\n\n    # view the results\n    result = job.result()\n    print(result.get_counts())\nMake sure you replace the THE_API_TOKEN_YOU_CREATED placeholder with the right API token.\n\nExecute the above script by running the commnad below.\n\npython main.py\n\nIt should return something like:\n\nResults OK\n{'0': 1024}\nNote: We get only 0’s because we are using the dummy cluster from quantify scheduler",
    "crumbs": [
      "Tutorials",
      "Adding authentication"
    ]
  },
  {
    "objectID": "tutorials/04_authorization.html",
    "href": "tutorials/04_authorization.html",
    "title": "Authorization",
    "section": "",
    "text": "This is how the Main Service Server (MSS) in tergite-frontend controls the user access to the quantum computer resource.\n\nWe control access to MSS, and its tergite-backend instances using two ways\n\nroles control basic access to auth-related endpoints e.g. project creation, token management etc.\nprojects control access to all other endpoints. To create a job, or get its results etc, one must be attached to a project that has more than zero QPU seconds.\n\nQPU seconds are the number of seconds a project’s experiments are allocated on the quantum computer.\nQPU seconds can be increased, decreased etc., but no job can be created without positive QPU seconds.\nA job could run for longer than the allocated project QPU seconds but it may fail to update MSS of its results. A user must thus make sure their project has enough QPU seconds.\n\n\nHow Authorization Works\nHere is an interaction diagram of auth showcasing authentication via MyAccessID.\n\n\n\n\n\n\n\n\n\nInteraction diagram of auth showcasing MyAccessID\n\n\n\n\n\n\n\n\nInteraction diagram of auth showcasing MyAccessID\n\n\n\n Back to top",
    "crumbs": [
      "Tutorials",
      "Authorization"
    ]
  },
  {
    "objectID": "tutorials/06_resource_management.html",
    "href": "tutorials/06_resource_management.html",
    "title": "Resource Management with Puhuri",
    "section": "",
    "text": "Puhuri is an HPC resource management platform that could also be used to manage Quantum Computer systems.\nWe need to synchronize MSS’s resource management with that in Puhuri\nThe Puhuri Entity Layout",
    "crumbs": [
      "Tutorials",
      "Resource Management with Puhuri"
    ]
  },
  {
    "objectID": "tutorials/06_resource_management.html#flows",
    "href": "tutorials/06_resource_management.html#flows",
    "title": "Resource Management with Puhuri",
    "section": "Flows",
    "text": "Flows\nMore information about flows can be found in the puhuri Tergite flows tutorial\n\n\n\n\n\n\n\n\n\nSelecting resource to report on",
    "crumbs": [
      "Tutorials",
      "Resource Management with Puhuri"
    ]
  },
  {
    "objectID": "tutorials/06_resource_management.html#assumptions",
    "href": "tutorials/06_resource_management.html#assumptions",
    "title": "Resource Management with Puhuri",
    "section": "Assumptions",
    "text": "Assumptions\n\nWhen creating components in the puhuri UI, the ‘measurement unit’s set on the component are of the following possible values: ’second’, ‘hour’, ‘minute’, ‘day’, ‘week’, ‘half_month’, and ‘month’.",
    "crumbs": [
      "Tutorials",
      "Resource Management with Puhuri"
    ]
  },
  {
    "objectID": "tutorials/06_resource_management.html#how-to-start-the-puhuri-sync",
    "href": "tutorials/06_resource_management.html#how-to-start-the-puhuri-sync",
    "title": "Resource Management with Puhuri",
    "section": "How to Start the Puhuri Sync",
    "text": "How to Start the Puhuri Sync\n\nEnsure that the is_enabled = true in the [puhuri] table in your mss-config.toml file\nEnsure all other variables in the [puhuri] table in your mss-config.toml file are appropriately set e.g.\n\n[puhuri]\n# the URI to the Puhuri WALDUR server instance\n# Please contact the Puhuri team to get this.\nwaldur_api_uri = \"&lt;the URI to the Puhuri Waldur server&gt;\"\n# The access token to be used in the Waldur client [https://docs.waldur.com/user-guide/] to connect to Puhuri\n# Please contact the Puhuri team on how to get this from the UI\nwaldur_client_token = \"&lt;API token for a puhuri user who has 'service provider manager' role for our offering on puhuri&gt;\"\n# The unique ID for the service provider associated with this app in the Waldur Puhuri server\n# Please contact the Puhuri team on how to get this from the UI\nprovider_uuid = \"&lt;the unique ID for the service provider associated with this app in Puhuri&gt;\"\n# the interval in seconds at which puhuri is polled. default is 900 (15 minutes)\npoll_interval = \"&lt;some value&gt;\"\n\nIf you wish to start only the puhuri synchronization script without the REST API, run in your virtual environment:\n\npython -m api.scripts.puhuri_sync --ignore-if-disabled\n\nIn order to run both the REST API and this puhuri synchronization script, run in your virtual environment:\n\npython -m api.scripts.puhuri_sync --ignore-if-disabled & \\\n  uvicorn --host 0.0.0.0 --port 8000 api.rest:app  --proxy-headers\n\n\n\nPuhuri Layout\nSelecting resource to report on",
    "crumbs": [
      "Tutorials",
      "Resource Management with Puhuri"
    ]
  },
  {
    "objectID": "tutorials/08_quick_start_dummy_cluster.html",
    "href": "tutorials/08_quick_start_dummy_cluster.html",
    "title": "Quick Start with a dummy cluster",
    "section": "",
    "text": "Let’s attempt to setup the Tergite stack to run on a dummy cluster on your local machine.\nWe will not need an actual quantum computer. Take note, however, that the dummy cluster only returns 0 in its results.",
    "crumbs": [
      "Tutorials",
      "Quick Start with a dummy cluster"
    ]
  },
  {
    "objectID": "tutorials/08_quick_start_dummy_cluster.html#prerequisites",
    "href": "tutorials/08_quick_start_dummy_cluster.html#prerequisites",
    "title": "Quick Start with a dummy cluster",
    "section": "Prerequisites",
    "text": "Prerequisites\nYou may have to install these software if you don’t have them already installed.\n\nDocker +v23.0.5\nConda\nRedis\nMongoDb\nVisual Studio Code\nMongo compass",
    "crumbs": [
      "Tutorials",
      "Quick Start with a dummy cluster"
    ]
  },
  {
    "objectID": "tutorials/08_quick_start_dummy_cluster.html#setup-the-frontend",
    "href": "tutorials/08_quick_start_dummy_cluster.html#setup-the-frontend",
    "title": "Quick Start with a dummy cluster",
    "section": "Setup the Frontend",
    "text": "Setup the Frontend\n\nEnsure you have docker is running.\n\ndocker --help\nNote: for MacOS, start docker by running this command\nopen -a Docker\nNote: for Windows, start docker by running this command\nStart-Process \"C:\\Program Files\\Docker\\Docker\\Docker Desktop.exe\"\nNote: for Linux, start docker by running this command\nsudo systemctl start docker\n\nOpen another terminal\nClone the tergite-frontend repo\n\ngit clone https://github.com/tergite/tergite-frontend.git\n\nEnter the tergite-frontend folder\n\ncd tergite-frontend\n\nCreate an mss-config.toml file with visual studio code (or any other text editor).\n\ncode mss-config.toml\n\nUpdate the mss-config.toml with the following content\n\n# mss-config.toml\n\n# general configurations\n[general]\n# the port on which MSS is running\nmss_port = 8002\n# the port on which the websocket is running\nws_port = 6532\n# environment reflect which environment the app is to run in.\nenvironment = \"development\"\n# the host the uvicorn runs on.\n# During testing auth on 127.0.0.1, set this to \"127.0.0.1\". default: \"0.0.0.0\"\nmss_host = \"127.0.0.1\"\n\n[database]\n# configurations for the database\nname = \"testing\"\n# database URI\n# host.docker.internal resolves to the host's 127.0.0.1\n# see https://stackoverflow.com/questions/31324981/how-to-access-host-port-from-docker-container#answer-43541732\nurl = \"mongodb://host.docker.internal:27017\"\n\n[[backends]]\nname = \"loke\"\n# the URL where this backend is running\n# host.docker.internal resolves to the host's 127.0.0.1\n# see https://stackoverflow.com/questions/31324981/how-to-access-host-port-from-docker-container#answer-43541732\nurl = \"http://host.docker.internal:8000\"\n\n[auth]\n# turn auth OFF or ON, default=true\nis_enabled = false\ncookie_domain = \"127.0.0.1\"\ncookie_name = \"tergiteauth\"\n\n[[auth.clients]]\nname = \"github\"\nclient_id = \"some-github-obtained-client-id\"\nclient_secret = \"some-github-obtained-client-secret\"\nredirect_url = \"http://127.0.0.1:8002/auth/github/callback\"\nclient_type = \"github\"\nemail_regex = \"^(john\\\\.doe|jane|aggrey)@example\\\\.com$\"\nemail_domain = \"example.com\"\nroles = [\"admin\", \"user\"]\n\n[[auth.clients]]\nname = \"puhuri\"\nclient_id = \"some-puhuri-obtained-client-id\"\nclient_secret = \"some-puhuri-obtained-client-secret\"\nredirect_url = \"http://127.0.0.1:8002/auth/puhuri/callback\"\nclient_type = \"openid\"\nemail_regex = \"^(john\\\\.doe|jane)@example\\\\.com$\"\nemail_domain = \"example.com\"\nroles = [\"user\"]\nopenid_configuration_endpoint = \"https://proxy.acc.puhuri.eduteams.org/.well-known/openid-configuration\"\n\n# Puhuri synchronization\n# Puhuri is a resource management platform for HPC systems, that is also to be used for Quantum Computer's\n[puhuri]\n# turn puhuri synchronization OFF or ON, default=true\nis_enabled = false\n\nCreate a .env file with visual studio code (or any other text editor).\n\ncode .env\n\nUpdate the .env with the following content\n\n# .env\n\nMSS_PORT=8002\n\n# required\nENVIRONMENT=\"development\"\nMSS_URL=\"http://127.0.0.1:8002\"\nGRAFANA_LOKI_URL=http://127.0.0.1:3100/loki/api/v1/push\nLOKI_LOGGER_ID=some-generic-id\n\n# docker LOGGING_DRIVER can be journald, json-file, local etc.\nLOGGING_DRIVER=json-file\n# image versions:\n# Note: If you ever want the images to be rebuilt,\n# you have to change the app version numbers here\n# before running \"docker compose up\"\nMSS_VERSION=v0.0.1\nDASHBOARD_VERSION=v0.0.1\nPROMTAIL_VERSION=2.8.3\n\nFor Linux: open MongoDB configurations file\n\ncode /etc/mongod.conf\n\nFor Linux: Replace the contents that config file with the following:\n\n# mongod.conf\n\n# for documentation of all options, see:\n#   http://docs.mongodb.org/manual/reference/configuration-options/\n\n# Where and how to store data.\nstorage:\n  dbPath: /var/lib/mongodb\n#  engine:\n#  wiredTiger:\n\n# where to write logging data.\nsystemLog:\n  destination: file\n  logAppend: true\n  path: /var/log/mongodb/mongod.log\n\n# network interfaces\nnet:\n  port: 27017\n  bindIp: 0.0.0.0\n\n\n# how the process runs\nprocessManagement:\n  timeZoneInfo: /usr/share/zoneinfo\n\n#security:\n\n#operationProfiling:\n\n#replication:\n\n#sharding:\n\n## Enterprise-Only Options:\n\n#auditLog:\n\nFor Linux: restart mongod service and make sure that it’s active\n\nsudo service mongod restart\nsudo service mongod status\n\nOpen the Mongo compass application and connect to the default local mongo database\n\n\n\n\n\n\n\n\n\n\n\n\nCreate a new mongo database called “testing” that contains a “backends” collection.\n\n\n\n\n\n\n\n\n\n\n\n\nDelete the old docker images of “tergite/tergite-mss”, “tergite/tergite-dashboard” from docker if they exist.\n\ndocker rmi -f tergite/tergite-mss:v0.0.1\ndocker rmi -f tergite/tergite-dashboard:v0.0.1\n\nTo Run the services, use the fresh-docker-compose.yml.\n\ndocker compose -f fresh-docker-compose.yml up -d\n\nRemove any stale artefacts created during the docker build\n\ndocker system prune\n\nOpen your browser at\n\nhttp://localhost:8002 to see the MSS service\nhttp://localhost:3000 to see the Dashboard application\n\nTo view the status of the services, run:\n\ndocker compose -f fresh-docker-compose.yml ps\n\nTo stop the services, run:\n\ndocker compose -f fresh-docker-compose.yml stop\n\nTo remove stop the services and remove their containers also, run:\n\ndocker compose -f fresh-docker-compose.yml down\n\nTo view logs of the docker containers to catch some errors, use:\n\ndocker compose -f fresh-docker-compose.yml logs -f\nsee more at https://docs.docker.com/reference/cli/docker/compose/logs/\n\nEnsure that the services are running. If they are not, restart them.\n\ndocker compose -f fresh-docker-compose.yml up -d",
    "crumbs": [
      "Tutorials",
      "Quick Start with a dummy cluster"
    ]
  },
  {
    "objectID": "tutorials/08_quick_start_dummy_cluster.html#setup-the-backend",
    "href": "tutorials/08_quick_start_dummy_cluster.html#setup-the-backend",
    "title": "Quick Start with a dummy cluster",
    "section": "Setup the Backend",
    "text": "Setup the Backend\n\nEnsure you have conda installed. (You could simply have python +3.12 installed instead.)\nEnsure you have the Redis server running.\n\nredis-server\n\nOpen terminal.\nClone the tergite-backend repo\n\ngit clone https://github.com/tergite/tergite-backend.git\n\nCreate conda environment\n\nconda create -n bcc -y python=3.12\nconda activate bcc\n\nInstall dependencies\n\ncd tergite-backend\npip install -r requirements.txt\n\nCreate an .env file with visual studio code (or any other text editor).\n\ncode .env\n\nUpdate .env file to have the following content\n\n# .env\nAPP_SETTINGS=development\nIS_AUTH_ENABLED=False\n\nDEFAULT_PREFIX=loke\nSTORAGE_ROOT=/tmp\nLOGFILE_DOWNLOAD_POOL_DIRNAME=logfile_download_pool\nLOGFILE_UPLOAD_POOL_DIRNAME=logfile_upload_pool\nJOB_UPLOAD_POOL_DIRNAME=job_upload_pool\nJOB_PRE_PROC_POOL_DIRNAME=job_preproc_pool\nJOB_EXECUTION_POOL_DIRNAME=job_execution_pool\n\n# Main Service Server\nMSS_MACHINE_ROOT_URL=http://localhost:8002\nMSS_PORT=8002\n\n# Backend Control computer\nBCC_MACHINE_ROOT_URL=http://localhost:8000\nBCC_PORT=8000\n\nEXECUTOR_TYPE=quantify\n\nLOG_LEVEL=INFO\nDEBUG=false\nUVICORN_LOG_LEVEL=info\n\nCreate an quantify-config.json file with visual studio code (or any other text editor).\n\ncode quantify-config.json\n\nUpdate the quantify-config.json with the following content\n\n{\n    \"config_type\": \"quantify_scheduler.backends.qblox_backend.QbloxHardwareCompilationConfig\",\n    \"hardware_description\": {\n      \"cluster0\": {\n        \"instrument_type\": \"Cluster\",\n        \"ref\": \"internal\",\n        \"modules\": {\n          \"2\":  { \"instrument_type\": \"QCM_RF\"  },  \n          \"3\":  { \"instrument_type\": \"QCM_RF\"  },   \n          \"16\": { \"instrument_type\": \"QRM_RF\"  }    \n        }\n      }\n    },\n    \"hardware_options\": {\n      \"modulation_frequencies\": {\n        \"q00:mw-q00.01\":       { \"lo_freq\": 4.80e9 },\n        \"q01:mw-q01.01\":       { \"lo_freq\": 4.23e9 },\n  \n        \"q00_q01:fl-q00_q01.cz\": { \"lo_freq\": 3.90e9 },\n  \n        \"q00:res-q00.ro\":      { \"lo_freq\": 6.80e9 },\n        \"q01:res-q01.ro\":      { \"lo_freq\": 6.80e9 }\n      }\n    },\n    \"connectivity\": {\n      \"graph\": [\n        [\"cluster0.module2.complex_output_0\", \"q00:mw\"],\n        [\"cluster0.module2.complex_output_1\", \"q01:mw\"],\n  \n        [\"cluster0.module3.complex_output_0\", \"q00_q01:fl\"],\n  \n        [\"cluster0.module16.complex_output_0\", \"q00:res\"],\n        [\"cluster0.module16.complex_output_0\", \"q01:res\"]\n      ]\n    }\n  }\n  \n\nCreate an quantify-metadata.yml file with visual studio code (or any other text editor).\n\ncode quantify-metadata.yml\n\nUpdate the quantify-metadata.yml with the following content\n\n# Cluster used by the instrument coordinator\ncluster0:\n  instrument_type: Cluster\n  ip_address: 192.168.78.101  \n  is_dummy: True         \n  modules:\n    \"2\":  { instrument_type: QCM_RF }\n    \"3\":  { instrument_type: QCM_RF }\n    \"16\": { instrument_type: QRM_RF }\n\n# SPI rack for DC bias on the coupler\nspi_rack:\n  instrument_type: SPI-Rack\n  port: \"/dev/ttyACM0\"        \n  is_dummy: True\n\n  # Keys MUST be canonical coupler IDs you use in code (e.g., 'u0')\n  coupler_spi_mapping:\n    u0:\n      spi_module_number: 6\n      dac_name: \"dac0\"\n\nCreate a backend_config.toml file with visual studio code (or any other text editor).\n\ncode backend_config.toml\n\nUpdate the backend_config.toml with the following content.\n\n# backend_config.toml\n[general_config]\nname = \"loke\"\nis_active = true\ncharacterized = true\nopen_pulse = true\nsimulator = false\nversion = \"1.0.0\"\nonline_date = \"2024-10-09T00:00:00\"\nnum_qubits = 2\nnum_couplers = 1\nnum_resonators = 2\ndt = 1e-9\ndtm = 1e-9\n\n[device_config]\ndiscriminators = [ \"lda\" ]\nqubit_ids = [ \"q0\", \"q1\" ]\nmeas_map = [ [ 0 ], [ 1 ] ]\ncoordinates = [\n  [0, 0],\n  [1, 0]\n]\n\nqubit_parameters = [\n  \"id\",\n  \"x_position\",\n  \"y_position\",\n  \"xy_drive_line\",\n  \"z_drive_line\",\n  \"frequency\",\n  \"pi_pulse_amplitude\",\n  \"pi_pulse_duration\",\n  \"pulse_type\",\n  \"pulse_sigma\",\n  \"t1_decoherence\",\n  \"t2_decoherence\"\n]\nresonator_parameters = [\n  \"id\",\n  \"x_position\",\n  \"y_position\",\n  \"readout_line\",\n  \"acq_delay\",\n  \"acq_integration_time\",\n  \"frequency\",\n  \"pulse_delay\",\n  \"pulse_duration\",\n  \"pulse_type\",\n  \"pulse_amplitude\"\n]\n\ncoupler_parameters = [\n  \"id\",\n  \"frequency\",\n  \"frequency_detuning\",\n  \"anharmonicity\",\n  \"coupling_strength_02\",\n  \"coupling_strength_12\",\n  \"cz_pulse_amplitude\",\n  \"cz_pulse_dc_bias\",\n  \"cz_pulse_phase_offset\",\n  \"cz_pulse_duration_before\",\n  \"cz_pulse_duration_rise\",\n  \"cz_pulse_duration_constant\",\n  \"control_rz_lambda\",\n  \"target_rz_lambda\",\n  \"pulse_type\"\n]\n\n[device_config.discriminator_parameters]\nlda = [\n  \"coef_0\",\n  \"coef_1\",\n  \"intercept\"\n]\n\n# Single coupler between q0 and q1\n[device_config.coupling_dict]\nu0 = [\"q0\", \"q1\"]\n\n[gates.x]\ncoupling_map = [ [ 0, 1], [1, 0] ]\nqasm_def = \"gate x q { U(pi, 0, pi) q; }\"\nparameters = [ ]\n\nCreate a calibration.seed.toml file with visual studio code (or any other text editor).\n\ncode calibration.seed.toml\n\nUpdate the calibration.seed.toml file with the following content.\n\n[calibration_config]\n\n[calibration_config.units.qubit]\nfrequency = \"Hz\"\nt1_decoherence = \"s\"\nt2_decoherence = \"s\"\nanharmonicity = \"Hz\"\n\n[calibration_config.units.readout_resonator]\nacq_delay = \"s\"\nacq_integration_time = \"s\"\nfrequency = \"Hz\"\npulse_delay = \"s\"\npulse_duration = \"s\"\npulse_amplitude = \"\"\npulse_type = \"\"\n\n[calibration_config.units.coupler]\nfrequency = \"Hz\"\nfrequency_detuning = \"Hz\"\nanharmonicity = \"Hz\"\ncoupling_strength_02 = \"Hz\"\ncoupling_strength_12 = \"Hz\"\ncz_pulse_amplitude = \"\"\ncz_pulse_dc_bias = \"\"\ncz_pulse_phase_offset = \"rad\"\ncz_pulse_duration_before = \"s\"\ncz_pulse_duration_rise = \"s\"\ncz_pulse_duration_constant = \"s\"\ncontrol_rz_lambda = \"rad\"\ntarget_rz_lambda = \"rad\"\npulse_type = \"\"\n\n# -- Qubits --\n\n[[calibration_config.qubit]]\nid = \"q0\"\nfrequency = 4.80e9\nanharmonicity = -0.17e9\nt1_decoherence = 30e-6\nt2_decoherence = 28e-6\npi_pulse_amplitude = 0.03\npi_pulse_duration = 56e-9\npulse_sigma = 7e-9\npulse_type = \"Gaussian\"\n\n[[calibration_config.qubit]]\nid = \"q1\"\nfrequency = 4.23e9\nanharmonicity = -0.17e9\nt1_decoherence = 30e-6\nt2_decoherence = 28e-6\npi_pulse_amplitude = 0.03\npi_pulse_duration = 56e-9\npulse_sigma = 7e-9\npulse_type = \"Gaussian\"\n\n# -- Resonators --\n\n[[calibration_config.readout_resonator]]\nid = \"q0\"\nacq_delay = 0\nacq_integration_time = 1e-6\nfrequency = 6.80e9\npulse_delay = 0\npulse_duration = 1e-6\npulse_amplitude = 0.2\npulse_type = \"Square\"\n\n[[calibration_config.readout_resonator]]\nid = \"q1\"\nacq_delay = 0\nacq_integration_time = 1e-6\nfrequency = 6.80e9\npulse_delay = 0\npulse_duration = 1e-6\npulse_amplitude = 0.2\npulse_type = \"Square\"\n\n\n# -- Couplers -- \n\n[[calibration_config.coupler]]\nid = \"u0\"\nfrequency = 3.90e9\nfrequency_detuning = -15e6\nanharmonicity = -0.17e9\ncoupling_strength_02 = 70e6\ncoupling_strength_12 = 70e6\ncz_pulse_amplitude = 0.08\ncz_pulse_dc_bias = 0.002          \ncz_pulse_phase_offset = 0.0\ncz_pulse_duration_before = 80e-9\ncz_pulse_duration_rise = 25e-9\ncz_pulse_duration_constant = 350e-9\ncontrol_rz_lambda = 0.3\ntarget_rz_lambda = 0.6\npulse_type = \"wacqt_cz\"\n\n# -- Discriminators --\n\n[calibration_config.discriminators.lda.q0]\nintercept = -38.4344477840827\ncoef_0    = -98953.87504155144\ncoef_1    = -114154.48696231026\n\nRun start script\n\n./start_bcc.sh\n\nOpen your browser at http://localhost:8000/docs to see the interactive API docs",
    "crumbs": [
      "Tutorials",
      "Quick Start with a dummy cluster"
    ]
  },
  {
    "objectID": "tutorials/08_quick_start_dummy_cluster.html#run-an-experiment",
    "href": "tutorials/08_quick_start_dummy_cluster.html#run-an-experiment",
    "title": "Quick Start with a dummy cluster",
    "section": "Run an Experiment",
    "text": "Run an Experiment\n\nOpen another terminal\nCreate a new folder “tergite-test” and enter it\n\nmkdir tergite-test\ncd tergite-test\n\nCreate conda environment and activate it\n\nconda create -n tergite -y python=3.12\nconda activate tergite\n\nInstall qiskit and Tergite SDK by running the command below:\n\npip install qiskit\npip install tergite\n\nCreate a file main.py with visual studio code (or any other text editor).\n\ncode main.py\n\nUpdate the main.py file with the following content:\n\n# main.py\n\"\"\"A sample script doing a very simple quantum operation\"\"\"\nimport time\n\nimport qiskit.circuit as circuit\nimport qiskit.compiler as compiler\n\nfrom tergite import Job, Tergite\n\nif __name__ == \"__main__\":\n    # the Tergite API URL\n    API_URL = \"http://localhost:8002\"\n    # The name of the Quantum Computer to use from the available quantum computers\n    BACKEND_NAME = \"loke\"\n    # the name of this service. For your own bookkeeping.\n    SERVICE_NAME = \"local\"\n    # the timeout in seconds for how long to keep checking for results\n    POLL_TIMEOUT = 100\n\n    # create the Qiskit circuit\n    qc = circuit.QuantumCircuit(2, 2)\n    qc.h(0)\n    qc.cx(0, 1)\n    qc.measure(0, 0)\n    qc.measure(1, 1)\n\n    # create a provider\n    # provider account creation can be skipped in case you already saved\n    # your provider account to the `~/.qiskit/tergiterc` file.\n    # See below how that is done.\n    provider = Tergite.use_provider_account(service_name=SERVICE_NAME, url=API_URL)\n    # to save this account to the `~/.qiskit/tergiterc` file, add the `save=True`\n    # provider = Tergite.use_provider_account(service_name=SERVICE_NAME, url=API_URL, save=True)\n\n    # Get the Tergite backend in case you skipped provider account creation\n    # provider = Tergite.get_provider(service_name=SERVICE_NAME)\n    backend = provider.get_backend(BACKEND_NAME)\n    backend.set_options(shots=1024)\n\n    # compile the circuit\n    tc = compiler.transpile(qc, backend=backend)\n\n    # run the circuit\n    job: Job = backend.run(tc, meas_level=2, meas_return=\"single\")\n    job.wait_for_final_state(timeout=POLL_TIMEOUT)\n\n    # view the results\n    result = job.result()\n    print(result.get_counts())\n\nExecute the above script by running the command below.\n\npython main.py\n\nIt should return something like:\n\nResults OK\n{'00': 1024}\nNote: We get only 0’s because we are using the dummy cluster from quantify scheduler",
    "crumbs": [
      "Tutorials",
      "Quick Start with a dummy cluster"
    ]
  }
]